{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.4', '2.1.2+cu121')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Create a corpus containing only documents from the 'earn' category\n",
    "corpus = brown.sents()\n",
    "\n",
    "# Limit the corpus to the first 1000 sentences for demonstration purposes\n",
    "corpus = [[word.lower() for word in sentence] for sentence in corpus]\n",
    "corpus = corpus[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word sequences and unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4272"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another': 0, '12': 1, 'machinist': 2, ':': 3, 'aid-to-education': 4, 'exception': 5, 'bowed': 6, 'revelation': 7, 'continental': 8, 'planning': 9, 'affected': 10, 'weather': 11, 'cannot': 12, 'drifts': 13, 'fifteen': 14, 'total': 15, 'evacuate': 16, 'rejection': 17, 'ward': 18, 'monuments': 19, 'training': 20, 'intends': 21, 'criticized': 22, 'obtained': 23, 'texan': 24, 'sessions': 25, 'patrolman': 26, 'r-bergen': 27, 'cited': 28, 'suffered': 29, 'revision': 30, 'uses': 31, 'maurice': 32, 'tyler': 33, 'nuclear': 34, 'american': 35, 'pipeline': 36, 'impression': 37, 'spring': 38, 'we': 39, 'temporarily': 40, 'employment': 41, 'sledding': 42, 'remove': 43, 'opelika': 44, 'months': 45, 'moving': 46, 'amicable': 47, 'urges': 48, 'industrial': 49, 'four-year': 50, 'barnett': 51, 'scramble': 52, 'part-time': 53, 'warden': 54, 'sharp': 55, 'before': 56, 'm.': 57, 'hundreds': 58, 'near': 59, 'type': 60, 'seminole': 61, 'issue': 62, 'sending': 63, \"fall's\": 64, 'informed': 65, 'dating': 66, 'confidence': 67, 'press': 68, \"o'connor's\": 69, 'message': 70, 'pledges': 71, 'mostly': 72, 'am': 73, 'elect': 74, '70': 75, 'atlanta': 76, 'individuals': 77, 'selected': 78, 'responsibility': 79, 'tex.': 80, 'times': 81, 'nationally': 82, 'towards': 83, 'rush': 84, 'person': 85, 'inside': 86, 'meetings': 87, '!': 88, 'elaborate': 89, 'big': 90, 'sirens': 91, 'decent': 92, 'colquitt': 93, 'harriet': 94, 'karns': 95, 'scriptures': 96, '12,000': 97, 'popular': 98, 'compensation': 99, 'affirmation': 100, 'beame': 101, 'schedule': 102, 'plans': 103, 'losses': 104, 'letterman': 105, 'speakers': 106, 'clark': 107, '$172,000': 108, 're-set': 109, 'cornerstone': 110, 'alternative': 111, 'second': 112, 'realize': 113, 'yesterday': 114, 'njust': 115, '6.5': 116, 'mclemore': 117, 'battle': 118, '1945': 119, 'petition': 120, 'housing': 121, '30': 122, 'backs': 123, 'prop': 124, 'signal': 125, 'old': 126, 'march': 127, 'learn': 128, 'subjected': 129, 'offenses': 130, 'carry': 131, 'apply': 132, 'overthrow': 133, 'november': 134, 'modernizing': 135, 'master': 136, 'reelection': 137, 'prediction': 138, 'whole': 139, 'carmine': 140, 'emphasizes': 141, 'today': 142, 'couple': 143, 'paying': 144, 'bond': 145, 'details': 146, 'recorded': 147, 'workers': 148, 'valuable': 149, 'tore': 150, ',': 151, 'locale': 152, '65': 153, 'sounded': 154, 'efforts': 155, 'inquired': 156, 'adjustments': 157, 'know': 158, 'roosevelt': 159, '81': 160, 'investigating': 161, 'convey': 162, 'ad': 163, 'visits': 164, 'aggression': 165, 'bonding': 166, 'rebound': 167, 'does': 168, 'obligations': 169, \"berry's\": 170, 'responsible': 171, 'normalcy': 172, '102': 173, 'personnel': 174, 'displayed': 175, 'bites': 176, 'immediate': 177, 'dependence': 178, 'authorizing': 179, 'wall': 180, 'him': 181, 'assemble': 182, 'multi-million-dollar': 183, 'limited': 184, 'reader': 185, 'hopper': 186, 'intimidation': 187, 'response': 188, 'include': 189, 'northernmost': 190, '$100': 191, \"i'll\": 192, 'garza': 193, 'useless': 194, 'substitutionary': 195, 'keep': 196, 'hawksley': 197, 'available': 198, 'regular': 199, 'travelers': 200, 'aged-care': 201, \"association's\": 202, 'improving': 203, 'barred': 204, 'affair': 205, 'done': 206, 'domain': 207, 'maplecrest': 208, 'presidential': 209, 'son': 210, 'licensing': 211, 'ran': 212, 's.': 213, 'assemblies': 214, 'right': 215, 'conserve': 216, 'temple': 217, 'purchasing': 218, 'direct': 219, 'speech': 220, 'lawmakers': 221, 'and': 222, 'stolen': 223, 'retailers': 224, 'wise': 225, 'politicians': 226, 'strange': 227, 'recent': 228, 'insure': 229, 'acknowledge': 230, 'roads': 231, 'offer': 232, 'name': 233, 'erection': 234, 'widow': 235, \"eisenhower's\": 236, 'surprises': 237, 'daughter': 238, 'zimmerman': 239, 'dannehower': 240, 'drew': 241, 'fiscal': 242, 'union': 243, 'smell': 244, 'fate': 245, 'mathematics': 246, 'west': 247, 'students': 248, 'compromise': 249, 'builder': 250, 'football': 251, 'coping': 252, 'wording': 253, 'soon': 254, 'emerald': 255, 'view': 256, 'managers': 257, 'attracted': 258, 'grovers': 259, '11': 260, 'legislators': 261, 'usual': 262, '1,000': 263, 'aids': 264, 'professor': 265, 'act': 266, 'further': 267, 'goodis': 268, 'injured': 269, 'it': 270, 'whom': 271, '36': 272, 'bed': 273, 'hunter': 274, 'poll': 275, 'congenial': 276, 'any': 277, 'orange': 278, 'depends': 279, 'optimistic': 280, 'belief': 281, 'named': 282, 'chance': 283, 'superintendent': 284, 'corruption': 285, 'provide': 286, 'invasion': 287, 'atty.': 288, 'loyalty': 289, 'dewey': 290, 'twenty': 291, 'recommendations': 292, 'miracles': 293, 'thomas': 294, 'imposed': 295, 'sailing': 296, 'mack': 297, '31st': 298, 'say': 299, 'oum': 300, 'yearly': 301, \"party's\": 302, 'next': 303, 'deputies': 304, 'bad': 305, 'errors': 306, 'compare': 307, 'community': 308, 'property': 309, 'veiled': 310, 'transit': 311, 'emergency': 312, 'hotels': 313, '$45': 314, 'saving': 315, 'lamar': 316, 'inhabitants': 317, 'spending': 318, '3646': 319, 'sponsored': 320, '$5000': 321, 'proportionate': 322, '50': 323, 'gazing': 324, 'feature': 325, 'away': 326, '$457,000': 327, 'relative': 328, 'option': 329, 'arrests': 330, 'goal': 331, 'them': 332, '$3.5': 333, 'protected': 334, 'to': 335, 'boost': 336, '$10': 337, 'northern': 338, 'offenders': 339, 'third': 340, 'haltingly': 341, 'address': 342, \"wife's\": 343, '700': 344, 'show': 345, \"organization's\": 346, 'vindication': 347, 'leonard': 348, '350': 349, 'no.': 350, 'accounts': 351, 'adverse': 352, 'sway': 353, 'employes': 354, 'reviewed': 355, 'eye': 356, 'january': 357, 'appointing': 358, 'greeted': 359, 'eighteen': 360, 'inconclusive': 361, 'latter': 362, 'laboratory': 363, 'outline': 364, 'meantime': 365, 'service': 366, '180': 367, '1409': 368, 'establish': 369, 'absolutely': 370, 'main': 371, 'appraisers': 372, 'established': 373, 'mark': 374, 'officer': 375, 'printed': 376, 'governor': 377, 'anti-organization': 378, 'pricing': 379, 'confirm': 380, \"hughes'\": 381, 'sheets': 382, 'royal': 383, 'imprudently': 384, 'suit': 385, 'glow': 386, 'ave.': 387, 'bellows': 388, 'trucks': 389, 'babies': 390, 'permits': 391, '1948': 392, '8': 393, \"case's\": 394, \"caldwell's\": 395, 'methods': 396, 'favors': 397, 'owen': 398, 'quite': 399, 'underdeveloped': 400, 'effectively': 401, 'approved': 402, 'preserving': 403, 'undoubtedly': 404, 'detailed': 405, 'payment': 406, 'fill': 407, 'withstand': 408, 'shipped': 409, 'increased': 410, 'mention': 411, 'suspect': 412, 'presentation': 413, 'mrs.': 414, 'designated': 415, \"taxpayers'\": 416, 'forestry': 417, 'annual': 418, 'delegations': 419, 'left': 420, 'object': 421, 'toss': 422, 'widespread': 423, 'b.': 424, 'sitting': 425, 'rigging': 426, 'eliminates': 427, 'had': 428, \"administration's\": 429, 'remains': 430, 'independent': 431, 'sinless': 432, 'raising': 433, 'lodge': 434, 'ways': 435, 'enact': 436, 'teaching': 437, 'arkansas': 438, 'nomination': 439, 'wards': 440, 'springboard': 441, 'county-wide': 442, 'consultations': 443, 'mass': 444, 'tougher': 445, 'barnet': 446, 'often': 447, 'arrest': 448, 'wooden': 449, 'raymond': 450, 'most': 451, 'non-partisan': 452, 'asking': 453, 'disturbing': 454, 'essence': 455, 'pleaded': 456, \"court's\": 457, 'gesture': 458, 'jobs': 459, 'representatives': 460, 'heavy': 461, 'socialist': 462, 'legislator': 463, 'huffman': 464, 'reform': 465, 'appointee': 466, 'completed': 467, 'clerk': 468, 'december': 469, 'figures': 470, 'wind': 471, 'critics': 472, 'lacking': 473, 'atmosphere': 474, 'offices': 475, 'former': 476, 'tolerated': 477, 'berger': 478, 'testimony': 479, 'soule': 480, 'coalition': 481, 'green': 482, 'bitter': 483, 'transfer': 484, 'extern': 485, 'aided': 486, 'baton': 487, 'dentistry': 488, 'subpoenas': 489, \"master's\": 490, 'impetus': 491, 'unchanged': 492, 'discharge': 493, 'regime': 494, 'sound': 495, 'fruitful': 496, 'father': 497, 'strong': 498, 'house': 499, 'believing': 500, 'chambers': 501, 'collins': 502, '14.2': 503, 'rayburn': 504, 'st.': 505, 'upwards': 506, 'murville': 507, 'lord': 508, 'propose': 509, 'certificate': 510, 'remainder': 511, 'southwestern': 512, 'headed': 513, \"''\": 514, '1964': 515, 'entire': 516, '6': 517, 'dental': 518, 'east': 519, 'shouting': 520, 'meaning': 521, 'colonial': 522, 'proselytizing': 523, 'learning': 524, 'member': 525, 'decade': 526, 'spent': 527, 'underwrite': 528, '$5': 529, 'mayor': 530, 'asked': 531, 'harvard': 532, 'encouraging': 533, 'given': 534, 'regulate': 535, 'cope': 536, 'go': 537, 'steamed': 538, 'campaign': 539, 'within': 540, 'polls': 541, 'graduate': 542, 'various': 543, 'super': 544, 'something': 545, 'regrets': 546, 'faith': 547, 'specific': 548, 'weldon': 549, 'vote': 550, 'objection': 551, \"karns'\": 552, 'remained': 553, 'redistricting': 554, 'allotted': 555, 'successful': 556, 'copy': 557, 'close': 558, 'accredited': 559, 'ministers': 560, 'sheet': 561, 'clear': 562, 'gave': 563, 'there': 564, 'chief': 565, 'boston': 566, 'protested': 567, 'point': 568, 'commitments': 569, 'seeks': 570, 'd': 571, 'utility': 572, 'republic': 573, 'helped': 574, 'force': 575, 'policeman': 576, 'years': 577, 'introduction': 578, 'laughlin': 579, 'alternatives': 580, 'e.': 581, 'core': 582, 'elements': 583, 'brandeis': 584, 'contrasting': 585, 'changing': 586, 'testament': 587, 'expansion': 588, 'slate': 589, '95': 590, 'row': 591, \"nation's\": 592, 'compulsory': 593, 'succession': 594, 'notice': 595, 'beyond': 596, 'reelected': 597, 'threatened': 598, 'leaving': 599, 'bounded': 600, 'proposes': 601, 'traditional': 602, 'subpenas': 603, 'stay': 604, 'appoint': 605, 'assigned': 606, 'providing': 607, 'end': 608, 'rate': 609, 'statutes': 610, 'molvar': 611, 'policies': 612, 'health': 613, 'blame': 614, 'professors': 615, 'will': 616, 'heart': 617, 'lumber': 618, 'terms': 619, 'jersey': 620, 'replace': 621, 'retirements': 622, 'forming': 623, 'kremlin': 624, 'procedures': 625, 'adc': 626, '1953': 627, '200': 628, 'selfish': 629, 'give': 630, 'red': 631, 'responses': 632, 'changes': 633, '1-1/2': 634, 'insist': 635, 'locally': 636, \"canada's\": 637, 'sharply': 638, 'muster': 639, 'beaverton': 640, 'complete': 641, 'varani': 642, 'terry': 643, 'principles': 644, 'outspoken': 645, 'bible': 646, 'provocation': 647, '22': 648, \"town's\": 649, 'function': 650, 'struggle': 651, 'correspondents': 652, 'absolute': 653, 'powered': 654, 'masonic': 655, '1950': 656, 'leary': 657, 'corporation': 658, 'breakthrough': 659, 'company': 660, 'by-passing': 661, 'proposing': 662, 'text': 663, 'large': 664, 'published': 665, 'absorb': 666, 'confrontations': 667, 'correspondent': 668, 'essex': 669, 'impossibly': 670, 'usually': 671, 'instructor': 672, 'disaster': 673, 'its': 674, 'ankles': 675, 'highly': 676, 'ross': 677, 'trend': 678, '.': 679, 'unlike': 680, 'authority': 681, 'coordinate': 682, 'rates': 683, 'nature': 684, 'ally': 685, 'request': 686, 'majority': 687, 'revolt': 688, 'norm': 689, 'attempt': 690, 'memorial': 691, 'classes': 692, 'home': 693, 'strenuous': 694, \"governor's\": 695, 'too': 696, 'victim': 697, 'disagreed': 698, 'undermine': 699, 'navy': 700, 'neutralized': 701, 'building': 702, 'levy': 703, 'edges': 704, 'symbol': 705, 'impressive': 706, 'determine': 707, 'enter': 708, 'please': 709, 'famous': 710, 'forthcoming': 711, 'coming': 712, 'rapidly': 713, 'pratt': 714, 'classical': 715, 'debate': 716, 'featured': 717, 'across': 718, 'considering': 719, \"president's\": 720, 'unity': 721, 'deny': 722, 'asia': 723, 'failed': 724, 'she': 725, 'pearl': 726, 'expand': 727, 'congealed': 728, 'specifically': 729, 'noticeable': 730, 'partner': 731, 'balking': 732, 'dog': 733, 'complained': 734, '13': 735, 'signature': 736, 'inconsistencies': 737, 'battalion': 738, \"can't\": 739, 'highway': 740, 'those': 741, 'estimate': 742, 'credit': 743, 'advantage': 744, '330': 745, 'distribution': 746, 'lying': 747, 'liberal': 748, 'churchmen': 749, 'included': 750, 'hope': 751, 'present': 752, 'thus': 753, 'neighborhood': 754, \"it's\": 755, 'guessing': 756, 'estate': 757, 'visitor': 758, 'continuous': 759, 'own': 760, 'deeper': 761, 'lost': 762, '114': 763, 'frustrated': 764, 'got': 765, 'realization': 766, 'howard': 767, 'republicans': 768, 'fulbright': 769, \"we're\": 770, 'co-signers': 771, 'bearing': 772, 'mears': 773, 'pye': 774, 'happens': 775, 'raise': 776, 'resentment': 777, 'owner': 778, 'lee': 779, 'qualified': 780, '1,700': 781, 'bargaining': 782, 'announce': 783, 'gillis': 784, 'permitting': 785, 'congress': 786, 'passenger': 787, 'need': 788, 'expires': 789, 'commission': 790, 'branches': 791, 'attacks': 792, 'visit': 793, 'tomorrow': 794, 'nearly': 795, 'chicanery': 796, 'unanimous': 797, 'mayer': 798, 'residents': 799, 'understand': 800, 'filled': 801, '28th': 802, 'conclusion': 803, 'disapprove': 804, 'full-time': 805, 'john': 806, \"doesn't\": 807, 'feet': 808, 'see': 809, 'penalty': 810, 'misunderstanding': 811, 'goals': 812, 'worry': 813, 'academies': 814, 'dramatic': 815, 'original': 816, 'pro': 817, 'medicine': 818, \"members'\": 819, 'reached': 820, 'tension': 821, 'washington-oregon': 822, 'constitution': 823, 'f.': 824, 'receiving': 825, 'add': 826, 'harry': 827, 'underlying': 828, 'good': 829, 'public-school': 830, 'courts': 831, \"nato's\": 832, 'men': 833, 'cotten': 834, 'gay': 835, 'miscellaneous': 836, 'asks': 837, 'pirie': 838, 'welcomed': 839, 'reprimanded': 840, 'engaged': 841, 'investigations': 842, 'my': 843, \"mayor's\": 844, 'campaigning': 845, 'thoroughfare': 846, '520-acre': 847, 'little': 848, 'ticket': 849, 'springfield': 850, 'p.m.': 851, 'in': 852, 'position': 853, 'color': 854, 'six': 855, 'listed': 856, 'opposed': 857, 'review': 858, 'candidates': 859, 'longstanding': 860, 'uncertain': 861, 'living': 862, 'win': 863, 'chosen': 864, 'zurich': 865, 'strongly': 866, 'happen': 867, 'port': 868, 'jones': 869, 'holds': 870, 'telephone': 871, 'votes': 872, 'efficiency': 873, 'sales': 874, 'tactics': 875, 'angola': 876, 'organized': 877, 'extraordinarily': 878, '47': 879, 'raises': 880, 'formative': 881, 'club': 882, 'exert': 883, 'each': 884, 'conventional': 885, 'washington': 886, 'hyannis': 887, \"controller's\": 888, 'quiet': 889, 'even': 890, 'guards': 891, 'stressed': 892, 'if': 893, 'enemy': 894, 'excuses': 895, 'highways': 896, 'daniel': 897, 'nursing': 898, 'trenton': 899, 'paths': 900, 'receive': 901, 'hesitated': 902, 'regarded': 903, 'ill': 904, 'truth': 905, 'upi': 906, 'revenues': 907, 'listen': 908, 'spectators': 909, 'blaine': 910, 'courteous': 911, 'anyway': 912, 'policemen': 913, 'builders': 914, 'university': 915, 'dissension': 916, 'probation': 917, \"city's\": 918, '87-31': 919, 'management': 920, 'consistently': 921, 'amount': 922, 'exposed': 923, 'flow': 924, 'contracted': 925, 'families': 926, 'state': 927, 'paid-for': 928, 'pay': 929, 'over-all': 930, 'repaired': 931, 'customers': 932, 'titular': 933, 'higher': 934, 'earlier': 935, 'democrat': 936, 'itself': 937, 'writes': 938, 'decrease': 939, 'legal': 940, 'count': 941, 'occupying': 942, 'clearing': 943, 'information': 944, 'civil': 945, 'argued': 946, 'indispensable': 947, 'public': 948, 'enforced': 949, '$5,000': 950, 'walk': 951, 'showing': 952, '1955': 953, 'estimated': 954, 'diplomatic': 955, 'talk': 956, 'liquor': 957, 'all-woman': 958, 'westfield': 959, 'friendly': 960, 'june': 961, '4:30': 962, 'conspicuous': 963, 'torn': 964, 'portion': 965, 'witnessed': 966, 'already': 967, 'episode': 968, 'events': 969, 'periodic': 970, 'combined': 971, 'chester': 972, '18th': 973, 'enforce': 974, 'fees': 975, \"russia's\": 976, 'shown': 977, 'or': 978, '1942': 979, 'rico': 980, 'rural': 981, 'using': 982, '$344,000': 983, 'oak': 984, 'weeks': 985, 'deputy': 986, 'fortier': 987, 'sunday': 988, 'combating': 989, 'statues': 990, 'regretted': 991, 'reaction': 992, 'understates': 993, 'disclosures': 994, 'reads': 995, 'explained': 996, 'ivan': 997, 'platform': 998, 'consulted': 999, 'word': 1000, '21': 1001, 'central': 1002, 'abatuno': 1003, 'scott': 1004, 'licenses': 1005, 'conference': 1006, 'phone': 1007, 'dwellings': 1008, '21st': 1009, 'stresses': 1010, 'line': 1011, 'inspection': 1012, 'hotel': 1013, 'stocks': 1014, 'piersee': 1015, 'took': 1016, 'confirmed': 1017, 'developments': 1018, 'construction': 1019, 'foods': 1020, 'risk': 1021, 'seats': 1022, 'hospital': 1023, 'enlarge': 1024, 'airport': 1025, 'employer': 1026, 'develop': 1027, 'continue': 1028, 'freer': 1029, 'jesting': 1030, 'totaling': 1031, 'vandiver': 1032, 'vice': 1033, 'concluded': 1034, 'caused': 1035, 'cox': 1036, 'unnecessary': 1037, '33d': 1038, 'ivory': 1039, 'role': 1040, 'ptc': 1041, \"government's\": 1042, \"i'm\": 1043, 'society': 1044, 'parsons': 1045, 'appreciation': 1046, 'love': 1047, 'certifying': 1048, 'sum': 1049, 'undue': 1050, 'man': 1051, 'sapio': 1052, 'constitutional': 1053, 'army': 1054, 'youth': 1055, 'profit': 1056, \"council's\": 1057, 'awareness': 1058, 'delta': 1059, 'ranks': 1060, 'unmeritorious': 1061, 'waiting': 1062, 'part': 1063, 'crowd': 1064, 'adopted': 1065, 'illness': 1066, 'responding': 1067, '$500': 1068, 'insuring': 1069, 'franker': 1070, 'indicating': 1071, 'lots': 1072, 'appointments': 1073, 'shot': 1074, 'vehicles': 1075, 'with': 1076, 'seeking': 1077, 'witnessing': 1078, 'accused': 1079, 'pension': 1080, 'communists': 1081, 'shiflett': 1082, 'premium': 1083, 'knocked': 1084, 'afraid': 1085, 'publishing': 1086, 'pledge': 1087, 'indorsed': 1088, 'criminal': 1089, 'controversial': 1090, 'stimulatory': 1091, 'questioning': 1092, 'serve': 1093, 'hear': 1094, 'mission': 1095, 'reviewing': 1096, 'subject': 1097, 'formby': 1098, 'recommendation': 1099, 'gov.': 1100, 'writing': 1101, 'equipment': 1102, 'boosts': 1103, 'denomination': 1104, 'congressional': 1105, 'starting': 1106, 'than': 1107, 'relaxation': 1108, 'division': 1109, 'laos': 1110, 'praised': 1111, 'engineering': 1112, 'oppose': 1113, \"kennedy's\": 1114, 'clarification': 1115, '7': 1116, 'wait': 1117, 'wisdom': 1118, 'expended': 1119, '1961': 1120, 'gallup': 1121, 'planner': 1122, 'realistic': 1123, 'measure': 1124, 'attorney': 1125, 'enjoy': 1126, 'mac': 1127, 'pleas': 1128, 'metal': 1129, 'females': 1130, 'viewed': 1131, 'houses': 1132, 'contractual': 1133, 'overwhelmingly': 1134, 'accident': 1135, 'wreck': 1136, 'never': 1137, \"we'll\": 1138, 'leadership': 1139, 'wardens': 1140, 'advertising': 1141, '133': 1142, 'loses': 1143, 'industries': 1144, 'petitions': 1145, 'reps.': 1146, 'd.': 1147, 'vehicle': 1148, 'pro-western': 1149, 'verification': 1150, 'war': 1151, 'solicitor': 1152, 'last-minute': 1153, 'addressing': 1154, 'j.': 1155, 'bigger': 1156, '$67,000': 1157, 'erred': 1158, 'is': 1159, 'mouth': 1160, 'stated': 1161, 'insurance': 1162, 'fails': 1163, 'joe': 1164, 'because': 1165, 'texans': 1166, 'harlingen': 1167, 'oklahoma': 1168, 'wanted': 1169, 'resumption': 1170, 'field': 1171, 'came': 1172, 'folks': 1173, 'preferably': 1174, 'child': 1175, 'edith': 1176, 'progress': 1177, 'escheat': 1178, 'unable': 1179, 'starts': 1180, 'long': 1181, 'docile': 1182, 'tonight': 1183, 'slowed': 1184, 'audrey': 1185, 'expire': 1186, 'julian': 1187, 'drive-in': 1188, 'party': 1189, 'summerdale': 1190, 'continuance': 1191, 'beginning': 1192, 'carries': 1193, 'people': 1194, 'disproportionate': 1195, 'manifestations': 1196, 'okla.': 1197, 'intervenes': 1198, 'economic': 1199, 'norman': 1200, 'pro-communist': 1201, 'probable': 1202, 'influences': 1203, 'antonio': 1204, 'revulsion': 1205, 'guaranteed': 1206, 'miscount': 1207, 'letter': 1208, 'middle': 1209, 'princes': 1210, 'driveway': 1211, 'adnan': 1212, 'democracy': 1213, 'validated': 1214, 'pulled': 1215, 'bomb': 1216, \"women's\": 1217, 'sukarno': 1218, 'expansive': 1219, 'dr.': 1220, 'roberts': 1221, 'indicated': 1222, 'byrd': 1223, 'delegation': 1224, 'draft': 1225, 'extraordinary': 1226, 'where': 1227, 'surrounded': 1228, 'inflate': 1229, 'modernized': 1230, 'avenue': 1231, 'fight': 1232, 'impinging': 1233, 'seidel': 1234, 'mayoral': 1235, 'annually': 1236, '4-year': 1237, 'automobile': 1238, 'spots': 1239, 'relatives': 1240, 'woodland': 1241, 'feels': 1242, 'architecture': 1243, 'arouse': 1244, 'foster': 1245, 'agency': 1246, 'needed': 1247, '$2': 1248, 'gulf': 1249, 'entirely': 1250, 'labor': 1251, 'understanding': 1252, 'idea': 1253, 'issues': 1254, 'experience': 1255, 'succeeded': 1256, 'budgetary': 1257, \"couldn't\": 1258, 'wednesday': 1259, 'originally': 1260, 'seat': 1261, 'matters': 1262, 'ceremonial': 1263, 'talking': 1264, 'situation': 1265, 'ready': 1266, 'officers': 1267, 'communist': 1268, 'banks': 1269, 'forest': 1270, 'citizens': 1271, 'fare': 1272, 'followed': 1273, 'sunay': 1274, '637': 1275, 'existing': 1276, 'bronx': 1277, 'mediocre': 1278, 'halleck': 1279, 'cheshire': 1280, 'schools': 1281, 'praise': 1282, 'covered': 1283, 'cemal': 1284, 'clusters': 1285, 'by': 1286, 'deeply': 1287, 'garden': 1288, 'truths': 1289, 'fine': 1290, 'radio': 1291, '$40,000,000': 1292, 'elections': 1293, 'might': 1294, 'administrative': 1295, 'luncheon': 1296, 'alexander': 1297, 'remarkably': 1298, 'investigate': 1299, 'search': 1300, 'abuse': 1301, 'should': 1302, 'far': 1303, 'gradually': 1304, 'reside': 1305, 'souphanouvong': 1306, 'servants': 1307, '23d': 1308, 'required': 1309, 'forces': 1310, 'young': 1311, 'shrugged': 1312, 'commented': 1313, 'sept.': 1314, 'cooperation': 1315, 'stores': 1316, 'more': 1317, 'fit': 1318, 'without': 1319, 'file': 1320, 'down-payments': 1321, 'garland': 1322, 'restrained': 1323, 'outside': 1324, 'noted': 1325, 'outcome': 1326, 'stanton': 1327, 'submitted': 1328, 'rally': 1329, 'vicious': 1330, 'leave': 1331, 'profound': 1332, 'zone': 1333, '182': 1334, 'dollars': 1335, 'sentence': 1336, 'ullman': 1337, 'send': 1338, 'special-interest': 1339, 'edwin': 1340, 'among': 1341, 'factors': 1342, 'reiterating': 1343, 'overhauling': 1344, 'four': 1345, 'became': 1346, 'effort': 1347, 'depositors': 1348, 'combat': 1349, 'forty-year': 1350, 'significance': 1351, 'casualty': 1352, 'stansbery': 1353, 'vagueness': 1354, 'they': 1355, \"department's\": 1356, 'top': 1357, 'altho': 1358, 'setting': 1359, 'surveys': 1360, 'reservoirs': 1361, 'economize': 1362, 'tentative': 1363, 'says': 1364, 'started': 1365, 'motorists': 1366, 'other': 1367, 'low': 1368, 'storage': 1369, 'scholastics': 1370, 'free': 1371, 'massachusetts': 1372, \"i'd\": 1373, 'statement': 1374, 'organizing': 1375, 'visited': 1376, 'plainfield': 1377, '$157,460': 1378, '169': 1379, 'regarding': 1380, 'may': 1381, 'flying': 1382, 'salter': 1383, 'clements': 1384, '``': 1385, 'la': 1386, 'opening': 1387, 'table': 1388, 'draw': 1389, 'dormitories': 1390, 'delaney': 1391, 'vitality': 1392, 'berlin': 1393, 'denied': 1394, 'questioned': 1395, 'however': 1396, 'charles': 1397, 'absorbed': 1398, 'testifies': 1399, '$115,000': 1400, 'brokers': 1401, 'mandatory': 1402, \"saturday's\": 1403, 'bid': 1404, 'billion': 1405, 'observing': 1406, 'land': 1407, 'fund': 1408, 'transcended': 1409, 'solve': 1410, 'affiliations': 1411, 'associate': 1412, 'expressed': 1413, 'pennsylvania': 1414, 'money': 1415, 'matter': 1416, 'lafayette': 1417, 'voter': 1418, 'presides': 1419, 'consequently': 1420, 'launched': 1421, 'effective': 1422, 'best': 1423, 'career': 1424, 'small-town': 1425, 'gordon': 1426, 'threats': 1427, 'pending': 1428, '(': 1429, 'honest': 1430, 'college': 1431, 'research': 1432, 'october': 1433, 'representation': 1434, 'precincts': 1435, 'referendum': 1436, 'divorce': 1437, 'facing': 1438, 'wayne': 1439, \"formby's\": 1440, 'riverside': 1441, 'views': 1442, 'salvatore': 1443, 'means': 1444, 'closeness': 1445, 'sanantonio': 1446, 'bring': 1447, 'demanded': 1448, 'lever': 1449, 'rob': 1450, 'aroused': 1451, 'placing': 1452, 'area': 1453, '$1': 1454, 'improve': 1455, 'beach': 1456, 'developing': 1457, 'hospital-care': 1458, 'coliseum': 1459, 'lt.': 1460, 'friday': 1461, 'regents': 1462, 'depending': 1463, 'geographical': 1464, 'doctor': 1465, 'european': 1466, 'recipients': 1467, 'nobody': 1468, 'court': 1469, 'brought': 1470, 'cars': 1471, 'rev.': 1472, 'tell': 1473, 'age': 1474, 'feeling': 1475, '61st': 1476, 'r.': 1477, 'multnomah': 1478, 'mental': 1479, 'beaumont': 1480, 'explains': 1481, 'just': 1482, 'misconstrued': 1483, 'greece': 1484, \"daniel's\": 1485, 'diem': 1486, 'spoke': 1487, 'key': 1488, 'criticisms': 1489, 'types': 1490, 'base': 1491, 'hospitals': 1492, 'representative': 1493, 'sw': 1494, 'excess': 1495, 'determination': 1496, '$8,555': 1497, 'ralph': 1498, 'anticipated': 1499, 'harold': 1500, 'attractive': 1501, 'satisfied': 1502, 'identical': 1503, 'ratcliff': 1504, 'cod': 1505, '4911': 1506, 'added': 1507, 'modifications': 1508, 'aj': 1509, 'english': 1510, 'saw': 1511, 'grocery': 1512, 'neutralist': 1513, 'farms': 1514, 'parts': 1515, 'reynolds': 1516, 'bill': 1517, 'require': 1518, 'really': 1519, 'industry': 1520, \"another's\": 1521, 'tremendous': 1522, 'neutral': 1523, 'assist': 1524, 'nato': 1525, 'kaplan': 1526, 'coordinator': 1527, 'found': 1528, 'quick': 1529, 'montgomery': 1530, 'reason': 1531, '$4,800': 1532, 'tractor': 1533, 'awarding': 1534, 'benefits': 1535, 'reducing': 1536, 'hastened': 1537, 'samoa': 1538, 'goes': 1539, 'declares': 1540, 'fewer': 1541, 'beaming': 1542, 'letters': 1543, 'introduced': 1544, 'provided': 1545, 'against': 1546, 'confessing': 1547, '1923': 1548, 'presented': 1549, 'extremely': 1550, 'reward': 1551, 'marines': 1552, 'covering': 1553, 'registration': 1554, 'contingency': 1555, '1600': 1556, 'small': 1557, 'hit-run': 1558, 'fiasco': 1559, 'turned': 1560, 'fits': 1561, 'suffrage': 1562, 'separation': 1563, 'ease': 1564, 'nightmare': 1565, 'himself': 1566, 'enactment': 1567, 'elevated': 1568, 'los': 1569, 'enthusiastic': 1570, 'increases': 1571, 'trouble': 1572, 'aug.': 1573, 'opportunity': 1574, 'carey': 1575, 'lawrence': 1576, 'sherman': 1577, 'gas': 1578, 'acting': 1579, 'one-fourth': 1580, 'fellow': 1581, 'operated': 1582, 'screvane': 1583, 'help': 1584, 'explain': 1585, 'repairs': 1586, 'tenor': 1587, 'junta': 1588, 'together': 1589, 'discrimination': 1590, 'post': 1591, 'salary': 1592, 'vouchers': 1593, 'consecutive': 1594, 'staggered': 1595, 'stronger': 1596, 'commerce': 1597, 'severely': 1598, 'twice': 1599, 'manager': 1600, 'edward': 1601, 'squeezed': 1602, 'model': 1603, 'administration': 1604, '500': 1605, 'de': 1606, 'series': 1607, 'normal': 1608, 'executive': 1609, 'tied': 1610, 'going': 1611, 'advance': 1612, 'token': 1613, 'voluntarily': 1614, '$1,000': 1615, 'statements': 1616, 'legitimate': 1617, 'question': 1618, 'fired': 1619, 'acres': 1620, 'fifty-three': 1621, 'juries': 1622, 'welfare': 1623, 'constituted': 1624, 'speaker': 1625, 'conservatives': 1626, 'artist': 1627, 'supervisor': 1628, 'remarked': 1629, 'territories': 1630, 'misuse': 1631, 'seen': 1632, 'committeewoman': 1633, 'changed': 1634, 'victory': 1635, 'kansas': 1636, 'defeat': 1637, 'honored': 1638, 'publicized': 1639, 'wider': 1640, 'municipalities': 1641, 'dissent': 1642, 'procurement': 1643, 'rhode': 1644, 'martin': 1645, '4.4': 1646, 'parimutuels': 1647, 'loans': 1648, 'agreement': 1649, 'session': 1650, 'p.': 1651, 'waters': 1652, 'constant': 1653, 'bridges': 1654, 'pockets': 1655, 'process': 1656, 'revolving': 1657, 'have': 1658, 'negative': 1659, 'clears': 1660, 'joint': 1661, 'fair': 1662, \"won't\": 1663, 'sent': 1664, 'factory': 1665, 'absence': 1666, 'sectors': 1667, 'the': 1668, 'size': 1669, \"commissioner's\": 1670, 'military': 1671, 'regulating': 1672, 'finally': 1673, 'cost': 1674, 'records': 1675, 'charged': 1676, 'firemen': 1677, 'questions': 1678, 'lines': 1679, 'l.': 1680, 'contact': 1681, 'rough': 1682, 'likely': 1683, 'room': 1684, 'lieberman': 1685, 'specified': 1686, 'politicos': 1687, 'worker': 1688, \"car's\": 1689, 'paso': 1690, 'africa': 1691, 'companionship': 1692, 'purpose': 1693, 'appointed': 1694, 'charge': 1695, 'appearing': 1696, 'ask': 1697, 'repair': 1698, \"cotten's\": 1699, 'unfair': 1700, 'joints': 1701, 'walter': 1702, 'rooming': 1703, 'aside': 1704, 'shriver': 1705, 'while': 1706, 'controller': 1707, 'jan.': 1708, 'spark': 1709, 'build-up': 1710, 'section': 1711, 'grounds': 1712, 'blue': 1713, 'math': 1714, 'circuit': 1715, 'for': 1716, 'mississippi': 1717, 'nominating': 1718, 'passed': 1719, 'attending': 1720, 'weary': 1721, 'lieutenant': 1722, 'backed': 1723, 'infiltrating': 1724, 'occupancy': 1725, 'sam': 1726, 'basis': 1727, 'general': 1728, 'duffy': 1729, 'respected': 1730, 'block': 1731, 'allies': 1732, 'hackett': 1733, 'hundred': 1734, 'bourcier': 1735, 'dropping': 1736, 'eugene': 1737, 'glad': 1738, 'flat': 1739, 'acre': 1740, 'composition': 1741, 'out': 1742, 'inspections': 1743, 'reply': 1744, 'implementation': 1745, 'enthusiasm': 1746, 'helm': 1747, 'origin': 1748, 'energy': 1749, 'bills': 1750, \"barnard's\": 1751, 'expenses': 1752, 'saba': 1753, 'plus': 1754, 'miss': 1755, 'mutual': 1756, 'cordial': 1757, 'clouded': 1758, 'steady': 1759, 'funds': 1760, 'landed': 1761, \"georgia's\": 1762, 'overcrowding': 1763, 'acceptance': 1764, 'addition': 1765, 'york': 1766, 'inspired': 1767, 'dec.': 1768, 'disclosed': 1769, 'student': 1770, 'headline': 1771, 'stake': 1772, 'cooperating': 1773, 'prince': 1774, 'squeeze': 1775, 'sulphur': 1776, 'princess': 1777, 'precedent': 1778, 'judges': 1779, 'roos': 1780, 'early': 1781, 'kennedy': 1782, 'rouge': 1783, '1946': 1784, 'sponsor': 1785, 'emerge': 1786, 'legislation': 1787, 'ballots': 1788, 'curious': 1789, 'members': 1790, 'mr.': 1791, 'fact': 1792, 'could': 1793, 'validity': 1794, 'transferred': 1795, \"didn't\": 1796, 'finance': 1797, 'immigrant': 1798, 'emphasis': 1799, 'eyes': 1800, 'approval': 1801, 'supt.': 1802, 'goldberg': 1803, 'rise': 1804, '$50': 1805, 'stark': 1806, 'nilsen': 1807, 'two-year': 1808, 'knew': 1809, 'nevertheless': 1810, 'current': 1811, 'governs': 1812, 'acts': 1813, 'sort': 1814, 'callan': 1815, 'restless': 1816, 'advocacy': 1817, 'temporary': 1818, 'law': 1819, 'johnston': 1820, 'district': 1821, 'spirit': 1822, \"mississippi's\": 1823, 'script': 1824, 'boun': 1825, 'standpoint': 1826, 'grant-in-aid': 1827, 'huge': 1828, 'bowden': 1829, 'adequately': 1830, 'innocence': 1831, 'apartments': 1832, 'relatively': 1833, 'retiring': 1834, 'authorities': 1835, 'authoritative': 1836, 'lives': 1837, 'dorsey': 1838, 'speed': 1839, 'car': 1840, 'sandman': 1841, 'smith': 1842, 'bypass': 1843, 'benefit': 1844, 'intentions': 1845, 'plant': 1846, 'return': 1847, 'consider': 1848, '900-student': 1849, 'gives': 1850, 'biggest': 1851, 'round': 1852, 'two-hour': 1853, 'facilities': 1854, 'live': 1855, 'rising': 1856, 'crime': 1857, 'violations': 1858, 'schwartz': 1859, 'july': 1860, 'declined': 1861, 'treaty': 1862, 'concerned': 1863, 'tried': 1864, 'apparently': 1865, 'worked': 1866, 'hearing': 1867, 'oath-taking': 1868, 'sets': 1869, 'license': 1870, 'exacerbated': 1871, '29-5': 1872, 'interlude': 1873, 'ended': 1874, 'hastily': 1875, 'heavily': 1876, 'financial': 1877, 'assuming': 1878, 'wrongful': 1879, 'employed': 1880, 'costly': 1881, 'reportedly': 1882, 'erected': 1883, 'viet': 1884, 'either': 1885, '1925': 1886, 'rescind': 1887, 'can': 1888, 'mary': 1889, 'pelham': 1890, 'ceremonies': 1891, 'frankford': 1892, 'sense': 1893, 'provides': 1894, 'india': 1895, 'inclination': 1896, 'forecasts': 1897, 'which': 1898, 'staff': 1899, 'brady': 1900, 'timetable': 1901, 'fighting': 1902, 'trim': 1903, 'average': 1904, 'heitschmidt': 1905, 'seekonk': 1906, 'commissioner': 1907, 'located': 1908, 'think': 1909, 'officials': 1910, 'adding': 1911, 'technical': 1912, 'dependency': 1913, 'working': 1914, 'davis': 1915, 'rule': 1916, 'personally': 1917, 'willamette': 1918, 'discharging': 1919, 'determined': 1920, 'democratic': 1921, 'react': 1922, 'married': 1923, 'time': 1924, 'unlikely': 1925, 'junior': 1926, 'ruling': 1927, 'passing': 1928, 'semipublic': 1929, 'economist': 1930, '1.5': 1931, 'corporations': 1932, 'cuban': 1933, 'overseas': 1934, 'bay': 1935, 'stature': 1936, 'notte': 1937, 'retirement': 1938, 'feared': 1939, \"o'neill\": 1940, 'souvanna': 1941, 'instances': 1942, 'onrush': 1943, 'gaynor': 1944, 'has': 1945, 'canal': 1946, 'w.m.': 1947, 'better': 1948, 'animal': 1949, 'portions': 1950, 'sell': 1951, 'desire': 1952, 'reduced': 1953, \"jersey's\": 1954, 'culbertson': 1955, 'retained': 1956, 'avoid': 1957, \"year's\": 1958, 'renewal': 1959, 'gursel': 1960, 'problems': 1961, 'claims': 1962, '74': 1963, 'ballot': 1964, 'full': 1965, 'rehabilitation': 1966, 'demand': 1967, 'successor': 1968, '--': 1969, 'cooperate': 1970, 'construed': 1971, 'irregularities': 1972, 'urged': 1973, 'case': 1974, 'david': 1975, 'kiwanis': 1976, 'populous': 1977, 'philosophy': 1978, 'same': 1979, 'sees': 1980, 'follow': 1981, 'slums': 1982, '9': 1983, 'ritiuality': 1984, 'dedication': 1985, 'reasonable': 1986, 'neighbors': 1987, 'nationwide': 1988, 'deaf': 1989, 'rescue': 1990, 'native': 1991, 'troublesome': 1992, 'five-year': 1993, 'previous': 1994, 'ban': 1995, 'desertion': 1996, 'vacancy': 1997, 'fee': 1998, 'modern': 1999, 'north': 2000, 'world': 2001, 'instead': 2002, 'requirements': 2003, 'committee': 2004, 'meeting': 2005, 'projects': 2006, '1937': 2007, 'boosting': 2008, 'were': 2009, 'jockeying': 2010, 'ideas': 2011, '$2,000': 2012, '$15,000': 2013, 'produced': 2014, 'allowed': 2015, 'project': 2016, '24': 2017, 'resignation': 2018, 'cut': 2019, 'election': 2020, 'eliminating': 2021, 'doubt': 2022, 'promised': 2023, 'canvassers': 2024, 'ahead': 2025, 'description': 2026, 'dollar': 2027, 'conflict': 2028, 'hatfield': 2029, 'states': 2030, 'interested': 2031, 'met': 2032, 'powers': 2033, 'societies': 2034, 'parties': 2035, 'proceed': 2036, 'neighboring': 2037, '$12': 2038, 'steel': 2039, 'audience': 2040, 'midmorning': 2041, 'factor': 2042, 'produce': 2043, 'eight': 2044, 'petty': 2045, 'these': 2046, 'department': 2047, 'attended': 2048, 'do': 2049, 'test': 2050, 'corrupt': 2051, 'deadline': 2052, 'term-end': 2053, 'here': 2054, 'hoodlums': 2055, 'tracks': 2056, 'universities': 2057, 'expanded': 2058, 'president': 2059, 'group': 2060, 'basically': 2061, 'plows': 2062, 'co-operative': 2063, 'absent': 2064, 'allowing': 2065, 'alliance': 2066, 'year': 2067, 'look': 2068, 'merchandising': 2069, 'repay': 2070, 'attend': 2071, 'influence': 2072, 'family': 2073, 'economy': 2074, 'william': 2075, 'watch': 2076, 'answer': 2077, 'income': 2078, 'defenders': 2079, 'modified': 2080, 'pace': 2081, 'prevent': 2082, 'somewhat': 2083, 'air': 2084, 'tusks': 2085, 'days': 2086, 'marching': 2087, 'board': 2088, '$12,192,865': 2089, 'very': 2090, 'el': 2091, 'external': 2092, 'correctness': 2093, 'carcass': 2094, 'her': 2095, 'conducted': 2096, \"secretary's\": 2097, 'joblessness': 2098, 'jr.': 2099, 'observers': 2100, 'settlement': 2101, 'bankrupt': 2102, 'perform': 2103, 'went': 2104, 'filing': 2105, 'bit': 2106, 'tower': 2107, 'builtin': 2108, 'stands': 2109, 'rules': 2110, 'control': 2111, 'breathes': 2112, 'determining': 2113, 'corp.': 2114, 'global': 2115, 'us': 2116, 'eight-year': 2117, 're-enactment': 2118, 'shall': 2119, 'forum': 2120, 'competing': 2121, 'extended': 2122, 'bordering': 2123, 'referred': 2124, 'violation': 2125, 'past': 2126, '$30': 2127, 'retarded': 2128, 'clarence': 2129, 'value': 2130, 'effectiveness': 2131, 'communities': 2132, 'agitating': 2133, 'colonialist': 2134, 'put': 2135, 'kept': 2136, 'insisted': 2137, 'ankara': 2138, 'allen': 2139, 'directed': 2140, 'weaver': 2141, 'thompson': 2142, 'mentally': 2143, 'war-ridden': 2144, 'headquarters': 2145, 'revised': 2146, 'violence': 2147, 'hike': 2148, 'expedient': 2149, 'opinion': 2150, 'interim': 2151, 'decisions': 2152, 'disposal': 2153, 'patronage': 2154, 'involving': 2155, 'uncommitted': 2156, 'timely': 2157, 'hoc': 2158, 'calmest': 2159, 'between': 2160, \"he's\": 2161, 'boyce': 2162, 'controversy': 2163, 'strengthen': 2164, 'functionary': 2165, 'requirement': 2166, 'globe': 2167, 'statewide': 2168, 'cedvet': 2169, 'his': 2170, 'twelve': 2171, 'warning': 2172, 'andrew': 2173, 'robert': 2174, 'fears': 2175, 'cd': 2176, 'retired': 2177, 'trust': 2178, 'repeatedly': 2179, '39': 2180, 'hands': 2181, \"you'll\": 2182, 'who': 2183, 'citation': 2184, 'supervision': 2185, 'reiterated': 2186, 'felt': 2187, 'number': 2188, 'stream': 2189, 'chain': 2190, 'expense': 2191, 'contempt': 2192, 'widely': 2193, 'specifications': 2194, 'shirking': 2195, 'coast': 2196, 'find': 2197, 'george': 2198, 'un': 2199, 'unhappy': 2200, 'rebel': 2201, 'nine': 2202, 'whipped': 2203, 'hopes': 2204, 'study': 2205, 'applause': 2206, '30,000,000': 2207, 'colleges': 2208, 'endorse': 2209, 'resigned': 2210, 'image': 2211, 'tax': 2212, 'around': 2213, 'south': 2214, 'voters': 2215, 'bids': 2216, 'was': 2217, 'currently': 2218, 'how': 2219, 'picking': 2220, 'donald': 2221, 'except': 2222, 'quarrel': 2223, 'equally': 2224, 'decency': 2225, 'outlay': 2226, 'handed': 2227, 'vehicular': 2228, 'contested': 2229, 'almost': 2230, 'prove': 2231, 'since': 2232, 'u.s.': 2233, 'departments': 2234, 'several': 2235, 'shortly': 2236, 'decried': 2237, 'speaking': 2238, '41': 2239, 'iron': 2240, 'neil': 2241, 'predict': 2242, 'entail': 2243, 'deal': 2244, 'brandt': 2245, 'spokesmen': 2246, 'actions': 2247, 'during': 2248, 'internal': 2249, 'conditioned': 2250, 'taxi': 2251, 'multiple': 2252, \"bill's\": 2253, 'hazards': 2254, 'priority': 2255, 'atomic': 2256, 'consolidated': 2257, 'willing': 2258, 'offense': 2259, 'interview': 2260, 'result': 2261, 'bloc': 2262, 'operate': 2263, 'livelihood': 2264, 'southeast': 2265, 'great': 2266, '$37,500': 2267, 'morton': 2268, 'source': 2269, 'extra': 2270, 'avoided': 2271, 'barnard': 2272, 'alpharetta': 2273, 'barbs': 2274, 'sagging': 2275, 'junior-senior': 2276, 'ratified': 2277, 'subcommittee': 2278, 'probably': 2279, 'precinct': 2280, 'coordination': 2281, 'miller': 2282, 'looking': 2283, 'tardiness': 2284, 'adminstration': 2285, 'sexton': 2286, 'justice': 2287, 'based': 2288, 'theme': 2289, 'caught': 2290, 'cross': 2291, \"gerosa's\": 2292, 'durwood': 2293, 'agriculture': 2294, 'seems': 2295, 'loan': 2296, 'stout': 2297, 'generally': 2298, 'monthly': 2299, '$6,100,000,000': 2300, 'loyal': 2301, 'minute': 2302, '300,000': 2303, 'stand': 2304, '$3,500': 2305, '25': 2306, 'respond': 2307, 'turkey': 2308, 'incumbent': 2309, 'hardly': 2310, 'tims': 2311, 'separate': 2312, 'neutralists': 2313, 'life': 2314, 'taxation': 2315, 'proposals': 2316, 'arms': 2317, 'reading': 2318, 'despite': 2319, 'strengthening': 2320, 'many': 2321, 'enforcement': 2322, 'read': 2323, 'into': 2324, 're-arguing': 2325, 'much': 2326, 'vocational': 2327, 'appear': 2328, 'sue': 2329, 'wheel': 2330, 'millions': 2331, 'services': 2332, 'tendency': 2333, 'geraldine': 2334, 'popularity': 2335, '540': 2336, 'excise': 2337, 'khrushchev': 2338, 'themselves': 2339, 'mitchell': 2340, 'administrators': 2341, 'fleet': 2342, '71': 2343, 'cape': 2344, 'averell': 2345, 'everything': 2346, 'ones': 2347, 'steered': 2348, 'condemning': 2349, 'burke': 2350, 'systems': 2351, 'pull': 2352, 't.': 2353, 'strickland': 2354, 'monday': 2355, 'up': 2356, 'day': 2357, 'increase': 2358, 'let': 2359, 'week': 2360, 'island': 2361, 'supermarkets': 2362, 'warren': 2363, 'hot': 2364, 'williams': 2365, 'contributing': 2366, 'tactical': 2367, 'element': 2368, 'eastland': 2369, 'amending': 2370, 'plea': 2371, 'cutting': 2372, '400,000,000': 2373, 'aged': 2374, 'city': 2375, 'toolmaker': 2376, 'discovered': 2377, 'machines': 2378, 'wherever': 2379, 'created': 2380, 'emotionally': 2381, 'appeals': 2382, 'naval': 2383, '10,000': 2384, 'jack': 2385, 'steeves': 2386, 'treasurer': 2387, 'unspecified': 2388, 'barbara': 2389, 'including': 2390, 'companies': 2391, 'o.': 2392, 'prejudicial': 2393, 'texas': 2394, 'score': 2395, 'juvenile': 2396, 'ranged': 2397, 'karol': 2398, 'task': 2399, 'vexing': 2400, 'pin': 2401, 'surprised': 2402, 'rigged': 2403, 'certain': 2404, 'churches': 2405, 'thing': 2406, 'eligible': 2407, \"rayburn's\": 2408, 'limit': 2409, 'fbi': 2410, 'provision': 2411, 'posts': 2412, 'registered': 2413, 'henry': 2414, 'encounter': 2415, 'handled': 2416, 'rights': 2417, 'why': 2418, 'filling': 2419, '4th': 2420, 'inspiring': 2421, 'thru': 2422, 'opinions': 2423, 'did': 2424, 'parkhouse': 2425, 'piracy': 2426, 'spends': 2427, 'mind': 2428, 'conceded': 2429, 'candor': 2430, 'step': 2431, 'house-cleaning': 2432, 'copeland': 2433, 'a': 2434, 'needs': 2435, 'thailand': 2436, 'counseling': 2437, 'scandals': 2438, 'corps': 2439, 'chemistry': 2440, 'james': 2441, 'geneva': 2442, 'waterfront': 2443, 'stepping': 2444, '5000': 2445, 'afford': 2446, 'item': 2447, '$4': 2448, 'virginia': 2449, 'illinois': 2450, 'credited': 2451, '6,000': 2452, 'savings': 2453, 'slogans': 2454, \"meyner's\": 2455, 'galveston': 2456, 'assured': 2457, 'automatically': 2458, 'eastwick': 2459, 'w.': 2460, 'tea': 2461, '92': 2462, 'noting': 2463, 'sargent': 2464, 'place': 2465, 'outgoing': 2466, 'decided': 2467, 'whereby': 2468, 'few': 2469, 'although': 2470, 'startled': 2471, 'movement': 2472, 'decries': 2473, 'natural': 2474, 'choices': 2475, 'honeymoon': 2476, 'parks': 2477, 'interviews': 2478, 'activity': 2479, 'sheriff': 2480, 'suited': 2481, 'relinquish': 2482, 'perfect': 2483, 'drive': 2484, 'ponies': 2485, 'job': 2486, 'co-operation': 2487, 'deterrent': 2488, 'malingering': 2489, 'opened': 2490, 'allocated': 2491, 'fire': 2492, '63': 2493, 'senior': 2494, 'fulfilled': 2495, 'sabbath': 2496, 'grady': 2497, 'half-brothers': 2498, 'essential': 2499, 'miles': 2500, 'light': 2501, 'agreed': 2502, 'attraction': 2503, 'history': 2504, 'special': 2505, 'promise': 2506, 'raised': 2507, 'programs': 2508, '7.5': 2509, 'removal': 2510, 'defeated': 2511, 'discourage': 2512, 'portland': 2513, 'of': 2514, 'grant': 2515, 'become': 2516, 'fulton': 2517, 'malcolm': 2518, 'comment': 2519, 'like': 2520, 'diplomat': 2521, 'inc.': 2522, 'used': 2523, 'confronting': 2524, 'thinks': 2525, 'burglary': 2526, 'acceptable': 2527, 'spokesman': 2528, 'operation': 2529, 'detriment': 2530, '29th': 2531, 'conservative': 2532, 'picture': 2533, 'directions': 2534, 'knauer': 2535, 'greeting': 2536, 'senate': 2537, 'transportation': 2538, 'lawyer': 2539, '1961-62': 2540, 'bureau': 2541, 'discussions': 2542, 'western': 2543, 'calls': 2544, 'incorporated': 2545, 'filed': 2546, 'aikin': 2547, 'springs': 2548, 'dissents': 2549, 'elected': 2550, 'hailed': 2551, 'joined': 2552, 'y.': 2553, '24th': 2554, 'threshold': 2555, 'males': 2556, 'stab': 2557, 'workshop': 2558, 'largest': 2559, 'prosecute': 2560, 'mass.': 2561, 'signers': 2562, 'resolution': 2563, 'cases': 2564, 'individual': 2565, 'blended': 2566, 'tossed': 2567, 'dilemma': 2568, 'length': 2569, 'feeble': 2570, 'patience': 2571, 'doubled': 2572, 'swipe': 2573, 'rd.': 2574, 'gift': 2575, 'advisement': 2576, 'congressmen': 2577, 'report': 2578, 'running': 2579, 'later': 2580, '29': 2581, 'columbia': 2582, 'receives': 2583, 'nam': 2584, 'austin': 2585, 'voluntary': 2586, 'theater': 2587, 'reports': 2588, 'nationalism': 2589, 'financed': 2590, 'solemnly': 2591, \"alliance's\": 2592, 'multiply': 2593, 'navigation': 2594, 'blueprints': 2595, 'appropriation': 2596, 'degree': 2597, 'constantinos': 2598, 'products': 2599, 'having': 2600, 'television': 2601, 'islands': 2602, 'schley': 2603, 'brief': 2604, 'quickly': 2605, ')': 2606, 'horse': 2607, 'victories': 2608, 'redevelopers': 2609, 'brooklyn': 2610, 'issued': 2611, 'soviet': 2612, 'perhaps': 2613, 'so': 2614, 'gotten': 2615, 'prevention': 2616, 'bleacher-type': 2617, 'population': 2618, 'million': 2619, 'oct.': 2620, 'pressure': 2621, 'dispute': 2622, 'archives': 2623, '$43,000': 2624, 'asian': 2625, 'gaining': 2626, 'equal': 2627, 'disclosure': 2628, 'recommended': 2629, 'hooked': 2630, 'segregation': 2631, 'less': 2632, 'pedigreed': 2633, 'martinelli': 2634, 'pistol': 2635, 'exact': 2636, 'school': 2637, 'maximum': 2638, \"yesterday's\": 2639, 'assertion': 2640, 'thanks': 2641, 'voting': 2642, 'affixed': 2643, 'street': 2644, 'keeping': 2645, 'admitted': 2646, 'driver': 2647, 'dinner': 2648, 'similar': 2649, 'supreme': 2650, 'obtaining': 2651, '75': 2652, 'lifeblood': 2653, \"you're\": 2654, 'reception': 2655, 'marshall': 2656, 'always': 2657, 'certainly': 2658, 'remarks': 2659, '16,000': 2660, 'helping': 2661, 'christ': 2662, 'cut-off': 2663, 'nov.': 2664, 'stays': 2665, 'sign': 2666, 'also': 2667, 'nothing': 2668, 'made': 2669, 'baptist': 2670, 'possibility': 2671, 'empire': 2672, 'a.m.': 2673, 'greatest': 2674, 'participating': 2675, 'amateurish': 2676, 'pushed': 2677, 'spearhead': 2678, 'compensated': 2679, '125,000': 2680, 'kas.': 2681, 'sanitary': 2682, 'smooth': 2683, '3.25': 2684, 'mcconnell': 2685, 'precipitated': 2686, 'threatening': 2687, 'betting': 2688, 'coup': 2689, 'sen.': 2690, 'elephants': 2691, 'deductible': 2692, 'claim': 2693, 'relieve': 2694, 'so-far': 2695, 'reconsideration': 2696, 'soldiers': 2697, 'ordinance': 2698, 'represents': 2699, 'administrator': 2700, 'requests': 2701, 'night': 2702, 'unconstitutional': 2703, 'veteran': 2704, 'paris': 2705, 'conspiracy': 2706, 'phouma': 2707, \"nugent's\": 2708, 'couve': 2709, 'reorganization': 2710, 'eligio': 2711, 'alleged': 2712, 'raiser': 2713, 'exploratory': 2714, 'april': 2715, 'safety': 2716, 'thought': 2717, 'appointment': 2718, 'guilty': 2719, 'shrines': 2720, 'subjects': 2721, 'climate': 2722, 'compelled': 2723, 'politics': 2724, 'representations': 2725, 'interesting': 2726, 'taxes': 2727, 'subsistence': 2728, 'ribbon': 2729, 'tragedies': 2730, 'completes': 2731, 'spend': 2732, 'i': 2733, 'would': 2734, 'moune': 2735, 'r-cape': 2736, 'emphasizing': 2737, 'connection': 2738, 'multi-family': 2739, 'countries': 2740, '1921': 2741, 'about': 2742, 'visiting': 2743, 'governments': 2744, 'unit': 2745, 'agenda': 2746, 'once': 2747, 'per': 2748, 'violated': 2749, 'then': 2750, 'succeed': 2751, 'richard': 2752, 'women': 2753, 'stripped': 2754, 'sensitive': 2755, 'illegitimacy': 2756, 'recipient': 2757, 'indemnity': 2758, 'detroit': 2759, 'false': 2760, 'church-state': 2761, 'bontempo': 2762, 'major': 2763, 'virgil': 2764, 'edgar': 2765, '100': 2766, 'restrain': 2767, 'norristown': 2768, 'as': 2769, 'handle': 2770, 'forbids': 2771, 'all': 2772, 'northwestern': 2773, 'wave': 2774, 'route': 2775, 'exclusive': 2776, 'felix': 2777, 'susceptible': 2778, 'watered': 2779, 'said': 2780, 'followers': 2781, 'expected': 2782, 'intervened': 2783, '58th': 2784, 'five': 2785, 'whose': 2786, 'birth': 2787, 'hess': 2788, 'every': 2789, 'inn': 2790, 'grasp': 2791, 'reported': 2792, 'build': 2793, 'ineffectual': 2794, 'future': 2795, 'extension': 2796, 'tangible': 2797, 'charges': 2798, 'democrats': 2799, 'returned': 2800, \"byrd's\": 2801, 'carved': 2802, 'techniques': 2803, 'losing': 2804, 'notarized': 2805, 'received': 2806, 'ledford': 2807, 'death': 2808, 'merger': 2809, 'concentrate': 2810, 'inadequate': 2811, 'organize': 2812, 'merchandise': 2813, 'g.': 2814, 'foes': 2815, 'athletic': 2816, 'behalf': 2817, 'mining': 2818, '$581,000': 2819, 'town': 2820, '2d': 2821, 'republican': 2822, 'sixth': 2823, 'additional': 2824, 'clash': 2825, \"republicans'\": 2826, 'conciliatory': 2827, 'encourage': 2828, 'issuance': 2829, 'split': 2830, 'defense': 2831, 'involve': 2832, 'revive': 2833, '$2,330,000': 2834, 'opponents': 2835, 'penalized': 2836, 'disappointment': 2837, 'advice': 2838, 'v.': 2839, 'railroad': 2840, 'set': 2841, 'united': 2842, 'date': 2843, 'assure': 2844, 'new': 2845, 'known': 2846, 'teamsters': 2847, 'buildings': 2848, \"leader's\": 2849, 'ambitious': 2850, 'move': 2851, 'belanger': 2852, 'permitted': 2853, 'ambiguous': 2854, 'aiding': 2855, 'fully': 2856, 'common': 2857, 'east-west': 2858, 'down': 2859, 'unanimously': 2860, \"body's\": 2861, 'half': 2862, 'come': 2863, 'melvin': 2864, 'behind': 2865, '$740,000': 2866, 'novel': 2867, 'everyone': 2868, 'connall': 2869, 'difficult': 2870, 'practicing': 2871, 'moreover': 2872, 'performance': 2873, '13th': 2874, 'portugal': 2875, 'graduation': 2876, 'arises': 2877, 'me': 2878, 'violate': 2879, 'exaltation': 2880, 'freeze': 2881, 'pleads': 2882, 'houston': 2883, 'younger': 2884, 'speak': 2885, 'keynote': 2886, '$88,000': 2887, 'locate': 2888, 'talked': 2889, 'form': 2890, 'stood': 2891, 'side': 2892, 'lao': 2893, 'fear': 2894, 'easy': 2895, 'doubling': 2896, 'played': 2897, 'rather': 2898, 'sheraton-biltmore': 2899, 'demonstrate': 2900, 'alienated': 2901, 'contract': 2902, 'tnt': 2903, 'term': 2904, 'duty': 2905, 'bulwark': 2906, 'customary': 2907, 'identified': 2908, 'loyalists': 2909, 'reforms': 2910, 'homes': 2911, 'offered': 2912, 'dallas': 2913, 'explicit': 2914, 'discredit': 2915, 'gifts': 2916, 'receptive': 2917, 'ap': 2918, 'requesting': 2919, 'choice': 2920, 'charter': 2921, 'rapid': 2922, 'loss': 2923, 'britain': 2924, 'greenfield': 2925, 'way': 2926, 'assistance': 2927, 'dependent': 2928, 'missionary': 2929, 'odds': 2930, 'induce': 2931, 'attachment': 2932, 'preserve': 2933, 'desperate': 2934, 'motor': 2935, 'sold': 2936, 'two-and-a-half-mile': 2937, 'forward': 2938, 'graft': 2939, 'formed': 2940, 'two': 2941, 'foreign': 2942, 'plunder': 2943, 'normally': 2944, '240': 2945, 'harris': 2946, '1910': 2947, 'power': 2948, 'exile': 2949, '1944': 2950, 'mates': 2951, 'held': 2952, 'resent': 2953, 'parents': 2954, 'political': 2955, 'wrong': 2956, 'taste': 2957, \"panel's\": 2958, 'stage': 2959, '$600': 2960, 'bellwood': 2961, 'rumford': 2962, 'dead': 2963, 'apart': 2964, 'taxpayers': 2965, 'truck': 2966, 'scheduled': 2967, 'privately': 2968, 'cocktail': 2969, 'contractors': 2970, '15': 2971, 'tuesday': 2972, 'orderly': 2973, 'being': 2974, '17': 2975, 'troubles': 2976, 'outlets': 2977, 'continued': 2978, '1920': 2979, 'begin': 2980, 'counties': 2981, 'serving': 2982, '$172,400': 2983, '42': 2984, 'join': 2985, '402': 2986, 'returning': 2987, 'decide': 2988, 'take': 2989, 'meager': 2990, 'professional': 2991, 'overly': 2992, 'indeed': 2993, \"school's\": 2994, 'wide': 2995, 'voiced': 2996, 'unemployment': 2997, 'bubenik': 2998, 'jury': 2999, 'louis': 3000, 'consultation': 3001, 'ethical': 3002, 'somebody': 3003, 'capacity': 3004, 'crisis': 3005, 'two-thirds': 3006, \"children's\": 3007, 'complication': 3008, 'figure': 3009, 'six-point': 3010, '60': 3011, 'boonton': 3012, 'advised': 3013, 'purchase': 3014, 'norway': 3015, 'kika': 3016, 'believed': 3017, 'joseph': 3018, 'after': 3019, 'september': 3020, 'definite': 3021, 'emphasize': 3022, 'nineteenth': 3023, 'minister': 3024, 'cent': 3025, 'newton': 3026, 'honor': 3027, 'firms': 3028, 'banker': 3029, 'anyone': 3030, 'creation': 3031, 'demands': 3032, '48': 3033, 'pronounced': 3034, 'solution': 3035, 'pricking': 3036, '1954': 3037, 'rumored': 3038, \"wouldn't\": 3039, 'hollowell': 3040, 'captain': 3041, 'badly': 3042, 'promotion': 3043, 'indicate': 3044, 'severly': 3045, 'poor': 3046, 'viewing': 3047, 'affairs': 3048, 'support': 3049, \"lowe's\": 3050, 'legislatures': 3051, '677': 3052, 'co.': 3053, 'analysis': 3054, 'payroll': 3055, 'guide': 3056, 'johnson': 3057, 'country': 3058, 'savannah': 3059, 'are': 3060, 'critical': 3061, 'erase': 3062, 'coordinating': 3063, 'direction': 3064, 'legislative': 3065, 'tripping': 3066, 'called': 3067, 'ore.': 3068, 'correct': 3069, 'prospect': 3070, 'larceny': 3071, 'necessary': 3072, 'opposite': 3073, 'plenary': 3074, 'commercial': 3075, 'eisenhower': 3076, 'others': 3077, 'socialized': 3078, 'illegal': 3079, 'oregon': 3080, 'been': 3081, 'use': 3082, 'teaches': 3083, 'official': 3084, 'proceedings': 3085, 'positions': 3086, 'front': 3087, 'detente': 3088, '150': 3089, 'quest': 3090, 'independents': 3091, '1920s': 3092, 'toward': 3093, '$60': 3094, 'sued': 3095, 'k.': 3096, 'taken': 3097, 'final': 3098, 'meanwhile': 3099, 'experienced': 3100, 'reduce': 3101, 'hard': 3102, 'resurrection': 3103, 'directly': 3104, 'campus': 3105, 'back': 3106, \"years'\": 3107, 'courses': 3108, 'hughes': 3109, 'reestablish': 3110, 'pretenses': 3111, 'thursday': 3112, 'prepared': 3113, '$3100': 3114, 'fortin': 3115, 'vice-president': 3116, 'minutes': 3117, \"phouma's\": 3118, 'doors': 3119, 'replies': 3120, '$22.50': 3121, 'america': 3122, 'consonance': 3123, 'horace': 3124, 'yes': 3125, 'ind.': 3126, 'hays': 3127, 'bet': 3128, 'utilities': 3129, 'high': 3130, 'event': 3131, 'whipple': 3132, 'saturday': 3133, 'owners': 3134, 'prices': 3135, 'approach': 3136, '9th': 3137, 'proven': 3138, 'pedestrian': 3139, '5.1': 3140, 'assent': 3141, 'revamped': 3142, 'courtroom': 3143, 'agreeing': 3144, 'latest': 3145, 'scholastic': 3146, 'educational': 3147, 'nurse': 3148, 'frame': 3149, 'hire': 3150, 'languages': 3151, 'chapters': 3152, 'crumlish': 3153, 'scene': 3154, 'fire-fighting': 3155, 'share': 3156, 'able': 3157, 'making': 3158, 'heard': 3159, 'dividing': 3160, 'immediately': 3161, 'prepayment': 3162, 'begins': 3163, 'space': 3164, 'termed': 3165, 'buchanan': 3166, 'now': 3167, 'gop': 3168, 'amid': 3169, 'honoring': 3170, 'esplanade': 3171, 'roof': 3172, 'districts': 3173, 'passage': 3174, 'laotians': 3175, 'stennis': 3176, 'born': 3177, 'panel': 3178, 'fourteen-nation': 3179, 'an': 3180, 'dealing': 3181, 'evacuation': 3182, 'century': 3183, 'business': 3184, 'violating': 3185, 'secretary-treasurer': 3186, 'organization': 3187, 'nixon': 3188, 'quickie': 3189, 'lip': 3190, 'water': 3191, 'plainview': 3192, 'unpredictability': 3193, 'investigation': 3194, 'secretariat': 3195, 'setback': 3196, 'rejected': 3197, 'angeles': 3198, 'outright': 3199, 'site': 3200, 'councilwoman': 3201, 'crump': 3202, '$26,000,000': 3203, 'record': 3204, 'shops': 3205, 'association': 3206, 'ordinary': 3207, 'pick': 3208, 'needy': 3209, 'explosion': 3210, 'mood': 3211, 'ike': 3212, 'at': 3213, 'genuine': 3214, 'forced': 3215, '$200,000': 3216, 'fines': 3217, 'gain': 3218, 'taunted': 3219, 'intervention': 3220, 'hood': 3221, 'greater': 3222, 'adopt': 3223, 'tracts': 3224, 'signs': 3225, 'actually': 3226, 'firm': 3227, 'barnes': 3228, 'establishment': 3229, '31': 3230, 'discredited': 3231, 'forswears': 3232, 'ago': 3233, 'discussed': 3234, 'hugh': 3235, 'inure': 3236, 'choose': 3237, 'program': 3238, 'flash': 3239, 'threat': 3240, '1940s': 3241, 'jim': 3242, 'square': 3243, 'amended': 3244, 'senator': 3245, 'sr.': 3246, 'enforcing': 3247, 're-elected': 3248, 'thousands': 3249, 'faced': 3250, 'berry': 3251, 'off': 3252, 'raymondville': 3253, 'assisted': 3254, 'surveyed': 3255, 'takes': 3256, 'indecisive': 3257, 'revenue': 3258, 'oslo': 3259, 'urgency': 3260, 'daily': 3261, 'cabinet': 3262, 'gubernatorial': 3263, 'impossible': 3264, 'items': 3265, 'described': 3266, 'ignition': 3267, 'elementary': 3268, 'terminate': 3269, 'placed': 3270, 'surveillance': 3271, 'sewer': 3272, 'revisions': 3273, 'primary': 3274, 'peace-loving': 3275, 'contracts': 3276, 'guilt': 3277, 'merely': 3278, '1': 3279, 'spotlight': 3280, 'moscow': 3281, 'mighty': 3282, 'castro': 3283, 'mayor-nominate': 3284, 'griffin': 3285, 'enterprise': 3286, 'prison': 3287, 'indonesia': 3288, 'al': 3289, 'experts': 3290, 'ala.': 3291, 'doctors': 3292, '$5,000,000': 3293, 'wagner': 3294, 'caldwell': 3295, 'camps': 3296, 'office': 3297, 'anonymous': 3298, 'historic': 3299, '4': 3300, 'puts': 3301, 'kingdom': 3302, 'costs': 3303, 'subpenaed': 3304, 'scattered': 3305, 'ride': 3306, 'monumental': 3307, 'respective': 3308, 'favor': 3309, 'governmental': 3310, 'educators': 3311, 'abe': 3312, 'dedicated': 3313, 'three': 3314, 'missions': 3315, 'cambridge': 3316, 'principal': 3317, 'advisers': 3318, 'contributions': 3319, 'change': 3320, 'pointed': 3321, 'bench': 3322, 'repealed': 3323, 'colleagues': 3324, 'moderate-income': 3325, 'boy': 3326, '$1,000,000,000': 3327, 'defends': 3328, 'barber': 3329, 'hopeful': 3330, 'viewpoint': 3331, 'announced': 3332, 'surplus': 3333, 'looked': 3334, 'keeps': 3335, 'pooling': 3336, 'estimates': 3337, 'want': 3338, 'assailed': 3339, 'job-seekers': 3340, 'distance': 3341, 'tattered': 3342, 'church': 3343, 'freedom': 3344, 'mainly': 3345, 'salinger': 3346, 'period': 3347, 'fall': 3348, 'real': 3349, 'contend': 3350, 'ever': 3351, \"country's\": 3352, 'so-called': 3353, 'hudson': 3354, 'this': 3355, 'con': 3356, 'recognized': 3357, \"boy's\": 3358, 'serious': 3359, 'u.': 3360, 'circulation': 3361, 'blocked': 3362, 'month': 3363, 'authorize': 3364, 'semester': 3365, 'completion': 3366, 'bonds': 3367, 'republican-controlled': 3368, 'bankers': 3369, 'calendar': 3370, 'intensified': 3371, 'still': 3372, 'basic': 3373, 'thousand': 3374, 'reserve': 3375, \"berger's\": 3376, 'believes': 3377, 'r-warren': 3378, 'driving': 3379, 'voted': 3380, 'seven-stories': 3381, 'call': 3382, 'counsel': 3383, 'feb.': 3384, 'shortcuts': 3385, 'negro': 3386, 'carried': 3387, 'balance': 3388, 'trims': 3389, 'following': 3390, 'getting': 3391, 'giveaway': 3392, 'narragansett': 3393, 'drop': 3394, 'active': 3395, 'sons': 3396, 'police': 3397, 'a.': 3398, 'preoccupied': 3399, 'blunders': 3400, 'crossroads': 3401, 'equitable': 3402, 'nations': 3403, 'morning': 3404, 'our': 3405, 'adoption': 3406, 'credits': 3407, 'clerical': 3408, 'attack': 3409, 'trends': 3410, 'looks': 3411, 'floor': 3412, 'kind': 3413, 'adjournment': 3414, 'examined': 3415, 'guardians': 3416, 'involved': 3417, 'maintenance': 3418, 'supported': 3419, 'regrouping': 3420, 'replaced': 3421, 'landscaped': 3422, 'sp.': 3423, 'walkways': 3424, 'channels': 3425, 'through': 3426, 'children': 3427, 'national': 3428, 'unite': 3429, 'tabb': 3430, 'catchers': 3431, 'instructed': 3432, 'burden': 3433, 'hoover': 3434, 'thruston': 3435, 'client': 3436, 'r': 3437, 'price': 3438, 'distasteful': 3439, '1913': 3440, 'cook': 3441, 'understood': 3442, 'enough': 3443, 'abraham': 3444, '10,000,000': 3445, 'peanut': 3446, 'paul': 3447, 'solidarity': 3448, 'latin': 3449, 'contracting': 3450, 'dogs': 3451, 'feis': 3452, 'sure': 3453, 'private': 3454, 'expert': 3455, 'judgment': 3456, 'intern': 3457, 'frequently': 3458, 'comes': 3459, 'allegations': 3460, 'focused': 3461, '$28': 3462, 'mean': 3463, 'calling': 3464, '1,119': 3465, 'cease-fire': 3466, 'salem': 3467, 'possible': 3468, 'coolest': 3469, 'attorneys': 3470, 'pardoned': 3471, 'gathering': 3472, 'harriman': 3473, 'reconstruction': 3474, \"d'etat\": 3475, 'confusion': 3476, 'allied': 3477, 'hilt': 3478, '$300,000,000': 3479, 'towel': 3480, '6-3-3': 3481, \"authority's\": 3482, 'areas': 3483, 'touch': 3484, 's.p.c.a.': 3485, \"monday's\": 3486, 'briefing': 3487, 'chapman': 3488, 'selections': 3489, 'ignored': 3490, 'else': 3491, \"denomination's\": 3492, 'manslaughter': 3493, 'authorized': 3494, 'germany': 3495, 'proud': 3496, 'ill.': 3497, 'week-end': 3498, 'title': 3499, 'ayes': 3500, 'assign': 3501, 'long-term': 3502, 'economically': 3503, 'pathet': 3504, 'tarrant': 3505, 'treated': 3506, 'urban': 3507, 'reduction': 3508, \"gladden's\": 3509, 'status': 3510, 'apparent': 3511, 'gun': 3512, 'wesley': 3513, 'consent': 3514, 'suggestion': 3515, 'acute': 3516, 'ninety-nine': 3517, 'pfaff': 3518, 'theodore': 3519, '3': 3520, 'snodgrass': 3521, 'witnesses': 3522, 'decision': 3523, 'c.': 3524, 'insult': 3525, 'interest': 3526, 'difficulties': 3527, 'religious': 3528, 'difference': 3529, 'circumstances': 3530, 'financing': 3531, 'knowing': 3532, 'mammoth': 3533, 'levitt': 3534, 'handful': 3535, 'headache': 3536, 'finances': 3537, 'challenge': 3538, 'clubs': 3539, 'michigan': 3540, 'conservation': 3541, 'canada': 3542, 'bachelor': 3543, 'inject': 3544, '1966': 3545, 'strength': 3546, 'director': 3547, 'talks': 3548, 'jesus': 3549, 'not': 3550, 'recovery': 3551, '$37': 3552, 'vacated': 3553, 'eliminate': 3554, 'plan': 3555, 'nugent': 3556, 'achieve': 3557, 'leaders': 3558, 'centralization': 3559, 'corner': 3560, 'admission': 3561, 'diety': 3562, 'adamant': 3563, 'must': 3564, 'reasons': 3565, 'signatures': 3566, '1865': 3567, 'all-out': 3568, 'handling': 3569, 'formerly': 3570, 'wrote': 3571, 'plains': 3572, 'killed': 3573, 'redevelopment': 3574, 'bryson': 3575, 'last': 3576, 'faster': 3577, 'firmer': 3578, 'protect': 3579, 'occupation': 3580, 'drain': 3581, 'related': 3582, '?': 3583, 'summer': 3584, 'grooming': 3585, 'strategy': 3586, 'names': 3587, 'science': 3588, 'saying': 3589, 'ground': 3590, 'product': 3591, 'intention': 3592, 'merit': 3593, 'harsh': 3594, '17,000': 3595, 'amendment': 3596, 'regard': 3597, 'manhattan': 3598, 'noon': 3599, 'halfway': 3600, 'alone': 3601, 'earliest': 3602, '100,000': 3603, 'proposal': 3604, 'delinquency': 3605, 'fined': 3606, 'your': 3607, 'accepting': 3608, 'volume': 3609, 'wage': 3610, '18': 3611, 'settle': 3612, 'shortage': 3613, 'agreements': 3614, 'appears': 3615, 'fund-raising': 3616, 'wexler': 3617, 'chorus': 3618, 'fixed': 3619, 'recommends': 3620, 'developed': 3621, 'flies': 3622, 'but': 3623, 'leaned': 3624, 'bloodstream': 3625, 'orleans': 3626, 'under-developed': 3627, 'gone': 3628, 'cities': 3629, 'effected': 3630, 'gross': 3631, 'voice': 3632, 'care': 3633, 'simplest': 3634, 'course': 3635, 'southern': 3636, 'largely': 3637, 'make': 3638, 'assistant': 3639, '10-year': 3640, 'favoring': 3641, 'fail': 3642, 'sheeran': 3643, 'their': 3644, 'unpleasant': 3645, 'whether': 3646, 'head': 3647, 'contractor': 3648, 'nitroglycerine': 3649, 'rep.': 3650, 'race': 3651, 'skills': 3652, 'objected': 3653, 'dominated': 3654, 'border': 3655, 'donations': 3656, 'bank': 3657, 'puddingstone': 3658, 'succeeds': 3659, 'worth': 3660, 'simultaneously': 3661, 'candidate': 3662, 'municipal': 3663, 'confronts': 3664, 'topics': 3665, 'lived': 3666, 'blast': 3667, 'morris': 3668, 'paper': 3669, 'dismissed': 3670, 'doxiadis': 3671, 'freeholder': 3672, 'denton': 3673, 'majorities': 3674, 'processes': 3675, '150,000,000': 3676, 'times-picayune': 3677, 'morse': 3678, 'results': 3679, 'falls': 3680, 'plowing': 3681, 'schrunk': 3682, 'led': 3683, 'hurdle': 3684, '250': 3685, 'paid': 3686, 'private-school': 3687, 'first': 3688, 'previously': 3689, 'locomotive': 3690, 'encouragement': 3691, 'screw': 3692, '1000': 3693, 'containing': 3694, 'defendants': 3695, 'team': 3696, 'rank': 3697, 'positive': 3698, 'diagnostic': 3699, 'friends': 3700, 'debut': 3701, 'especially': 3702, 'cruelty': 3703, 'buckley': 3704, 'evidence': 3705, 'scheduling': 3706, \"atlanta's\": 3707, 'intelligent': 3708, 'pedestrians': 3709, 'contention': 3710, 'traveled': 3711, 'nominee': 3712, 'strictly': 3713, 'grand': 3714, 'bush': 3715, 'coercion': 3716, 'puerto': 3717, 'labor-management': 3718, 'account': 3719, 'eventually': 3720, 'peoples': 3721, \"wasn't\": 3722, 'sought': 3723, 'seek': 3724, 'from': 3725, 'least': 3726, 'deliver': 3727, 'scale': 3728, 'structure': 3729, 'liberals': 3730, 'conduct': 3731, \"ordinary's\": 3732, '1963': 3733, 'percent': 3734, 'assembly': 3735, 'ours': 3736, 'moves': 3737, 'above': 3738, 'vantage': 3739, 'older': 3740, 'physics': 3741, 'san': 3742, 'start': 3743, 'harmony': 3744, 'device': 3745, 'requiring': 3746, 'culminating': 3747, 'method': 3748, 'list': 3749, 'devote': 3750, 'enacted': 3751, 'rival': 3752, \"rusk's\": 3753, 'directors': 3754, '$3': 3755, 'courageous': 3756, 'beliefs': 3757, 'faces': 3758, 'laws': 3759, 'breakdown': 3760, 'experimental': 3761, 'boos': 3762, 'inaugural': 3763, 'rekindling': 3764, 'transition': 3765, 'action': 3766, 'began': 3767, 'be': 3768, 'schooling': 3769, 'considered': 3770, 'georgia': 3771, 'accept': 3772, 'ghana': 3773, 'unnoticed': 3774, \"princes'\": 3775, 'accepted': 3776, 'contract-negotiation': 3777, '$15': 3778, 'until': 3779, 'scotch': 3780, 'technicians': 3781, 'station': 3782, 'game': 3783, 'cuba': 3784, 'rooms': 3785, 'concessionaires': 3786, 'formula': 3787, 'carson': 3788, 'performing': 3789, 'effect': 3790, 'institute': 3791, 'policy': 3792, 'achievement': 3793, 'permit': 3794, 'amounts': 3795, 'defray': 3796, 'diplomats': 3797, 'designed': 3798, 'basketball': 3799, 'causes': 3800, 'publicly': 3801, 'reporters': 3802, 'procedure': 3803, 'escape': 3804, 'hero': 3805, 'throw': 3806, 'legislature': 3807, 'dirksen': 3808, 'vincent': 3809, 'enunciate': 3810, 'danger': 3811, 'taking': 3812, 'hearings': 3813, 'newspapers': 3814, 'proposed': 3815, 'managing': 3816, 'macdonald': 3817, 'premier': 3818, 'outmoded': 3819, 'hours': 3820, 'increasing': 3821, 'michael': 3822, 'advisory': 3823, 'setup': 3824, 'retire': 3825, 'jackson': 3826, 'meyner': 3827, 'erupts': 3828, 'peace': 3829, '16': 3830, 'southern-republican': 3831, 'drafted': 3832, 'formally': 3833, 'hour': 3834, 'whatever': 3835, 'presentments': 3836, 'yet': 3837, 'pays': 3838, 'guam': 3839, 'everybody': 3840, 'newark': 3841, 'books': 3842, 'candidacy': 3843, 'kentucky': 3844, 'suggested': 3845, 'siding': 3846, 'scholarships': 3847, 'due': 3848, 'parochial': 3849, 'minor': 3850, 'gladden': 3851, '$15,000,000': 3852, 'growth': 3853, 'virgin': 3854, 'you': 3855, 'feeds': 3856, 'ierulli': 3857, 'abandoned': 3858, '$20': 3859, 'speeches': 3860, 'bar': 3861, 'under': 3862, 'jail': 3863, 'radios': 3864, 'run': 3865, 'replied': 3866, 'granted': 3867, 'resolve': 3868, '10': 3869, 'mortgages': 3870, 'representing': 3871, 'submarines': 3872, 'critic': 3873, 'examine': 3874, 'explanation': 3875, 'face': 3876, 'purposes': 3877, 'sharkey': 3878, 'soaring': 3879, 'capitol': 3880, 'served': 3881, 'initial': 3882, 'intellectually': 3883, 'wish': 3884, 'federation': 3885, '1311': 3886, 'midnight': 3887, 'suffragettes': 3888, 'rotary': 3889, 'scientist': 3890, 'crises': 3891, 'such': 3892, 'reference': 3893, 'again': 3894, 'enabling': 3895, 'fundamentally': 3896, 'sympathetic': 3897, 'explosive': 3898, 'committees': 3899, 're-election': 3900, 'aspects': 3901, 'drafts': 3902, 'particularly': 3903, 'technology': 3904, 'opposes': 3905, 'released': 3906, 'starter': 3907, 'reconvenes': 3908, 'feel': 3909, 'check': 3910, 'international': 3911, 'units': 3912, 'federal': 3913, 'doing': 3914, 'loyalist': 3915, '$2400': 3916, 'sens.': 3917, 'social': 3918, 'swearing': 3919, 'consulting': 3920, 'brod': 3921, 'lack': 3922, 'underground': 3923, 'knecht': 3924, 'stopped': 3925, 'groups': 3926, 'delegates': 3927, 'animated': 3928, 'works': 3929, 'county': 3930, 'pierre': 3931, 'aid': 3932, 'ex-gambler': 3933, 'ruled': 3934, 'open': 3935, 'pointing': 3936, 'dicks': 3937, 'guard': 3938, 'gainesville': 3939, 'free-for-all': 3940, 'white': 3941, 'one': 3942, 'vacancies': 3943, 'optimism': 3944, 'neither': 3945, 'wholesale': 3946, 'jimmie': 3947, 'play': 3948, 'remember': 3949, 'fundamentalism': 3950, '1962': 3951, 'conquer': 3952, 'entering': 3953, 'hand': 3954, 'miss.': 3955, '$451,500': 3956, '7,000': 3957, 'remedy': 3958, 'missing': 3959, 'pass': 3960, 'park': 3961, 'secret': 3962, 'modest': 3963, 'sufficient': 3964, 'sometime': 3965, 'mothers': 3966, 'prefer': 3967, 'won': 3968, 'god': 3969, 'deserves': 3970, 'outpatient': 3971, 'persuade': 3972, 'h.': 3973, 'grants': 3974, 'completely': 3975, 'work': 3976, 'worse': 3977, 'agencies': 3978, 'over': 3979, 'frank': 3980, 'level': 3981, 'waged': 3982, 'persons': 3983, 'failure': 3984, \"union's\": 3985, 'approve': 3986, \"state's\": 3987, 'prosecution': 3988, 'racial': 3989, '750': 3990, 'laid': 3991, 'unless': 3992, 'dumont': 3993, 'center': 3994, 'dismiss': 3995, 'target': 3996, '20': 3997, 'privilege': 3998, 'compete': 3999, 'desmond': 4000, 'combine': 4001, 'fires': 4002, 'prelude': 4003, 'single': 4004, \"portland's\": 4005, 'constantly': 4006, 'clearly': 4007, 'secretary': 4008, 'stalled': 4009, 'important': 4010, 'he': 4011, 'points': 4012, 'told': 4013, 'though': 4014, 'when': 4015, 'get': 4016, '40': 4017, 'range': 4018, '&': 4019, 'knowledge': 4020, 'seven': 4021, '8-4': 4022, 'enlarged': 4023, 'attention': 4024, 'what': 4025, 'expects': 4026, 'outlays': 4027, 'operates': 4028, 'justices': 4029, 'principals': 4030, 'chairman': 4031, 'hold': 4032, 'contended': 4033, 'irish': 4034, 'grover': 4035, 'bodily': 4036, 'weatherford': 4037, 'welled': 4038, 'practices': 4039, 'road': 4040, 'earned': 4041, 'plead': 4042, 'leading': 4043, 'weekend': 4044, 'league': 4045, 'breakups': 4046, 'along': 4047, 'clean': 4048, 'centralized': 4049, 'establishing': 4050, 'appeared': 4051, 'local': 4052, 'negotiations': 4053, 'allow': 4054, 'discussion': 4055, '90': 4056, 'constructing': 4057, 'intervals': 4058, 'turkish': 4059, 'totalitarian': 4060, '1958': 4061, 'superior': 4062, 'opposition': 4063, 'armed': 4064, 'patient': 4065, 'reama': 4066, 'jurors': 4067, 'according': 4068, 'only': 4069, 'clifford': 4070, 'hamlet': 4071, 'government': 4072, 'shopping': 4073, 'expanding': 4074, 'verbally': 4075, 'medical': 4076, 'hurrah': 4077, 'republicanism': 4078, 'americans': 4079, 'bombing': 4080, 'la.': 4081, 'n.': 4082, 'delay': 4083, 'cover': 4084, 'declared': 4085, 'senators': 4086, 'hemphill': 4087, 'matching': 4088, 'traffic': 4089, 'acquire': 4090, '42d': 4091, 'try': 4092, 'both': 4093, 'council': 4094, 'trade': 4095, 'depend': 4096, 'true': 4097, 'judge': 4098, 'vital': 4099, '$1,500': 4100, 'security': 4101, 'davenport': 4102, 'editing': 4103, 'arose': 4104, 'dynamic': 4105, 'allowances': 4106, 'no': 4107, 'upon': 4108, ';': 4109, 'budget': 4110, 'beating': 4111, 'development': 4112, 'scholarship': 4113, 'things': 4114, 'integration': 4115, 'gerosa': 4116, 'notion': 4117, 'closer': 4118, 'bankruptcy': 4119, 'apartment': 4120, 'well': 4121, '2': 4122, 'proper': 4123, 'leader': 4124, 'disposition': 4125, 'remark': 4126, 'weakness': 4127, 'marked': 4128, 'paradise': 4129, 'admitting': 4130, 'impair': 4131, 'insurgent': 4132, 'mo.': 4133, 'complementary': 4134, 'example': 4135, 'douglas': 4136, 'hartsfield': 4137, \"states'\": 4138, \"'\": 4139, \"texas'\": 4140, 'trial': 4141, 'manufacturers': 4142, 'setbacks': 4143, 'hard-fought': 4144, 'considerably': 4145, 'samuel': 4146, 'dean': 4147, 'conditions': 4148, 'personal': 4149, 'arthur': 4150, 'education': 4151, 'trimble': 4152, 'composed': 4153, 'pertained': 4154, 'wife': 4155, 'system': 4156, 'detach': 4157, 'operating': 4158, 'deficit': 4159, 'cash': 4160, \"france's\": 4161, 'goods': 4162, 'studied': 4163, 'preliminary': 4164, 'marvin': 4165, 'recommend': 4166, 'different': 4167, 'gen.': 4168, 'philadelphia': 4169, 'eminent': 4170, 'expect': 4171, 'accomodations': 4172, 'on': 4173, 'learned': 4174, 'seato': 4175, 'track': 4176, 'repeated': 4177, 'relations': 4178, '35': 4179, 'variety': 4180, 'assumed': 4181, 'firmly': 4182, 'ridge': 4183, 'that': 4184, 'saved': 4185, 'believe': 4186, 'nation': 4187, 'collects': 4188, 'belleville': 4189, 'tom': 4190, 'obtain': 4191, 'clinic': 4192, 'noes': 4193, 'order': 4194, 'teachers': 4195, 'teacher': 4196, '$25-a-plate': 4197, 'warned': 4198, 'load': 4199, 'steps': 4200, 'stems': 4201, 'none': 4202, 'announcement': 4203, 'congressman': 4204, 'enlarging': 4205, 'exported': 4206, 'meet': 4207, 'seemed': 4208, 'dumas': 4209, 'favorable': 4210, '1960': 4211, 'kill': 4212, 'alan': 4213, 'concern': 4214, 'teach': 4215, 'infallible': 4216, 'places': 4217, 'dwight': 4218, 'capital': 4219, 'territory': 4220, 'afternoon': 4221, 'domination': 4222, 'protocol': 4223, 'newly': 4224, 'friction': 4225, 'pertinent': 4226, 'trusting': 4227, 'kan.': 4228, 'observer': 4229, 'menderes': 4230, 'problem': 4231, 'extending': 4232, 'providence': 4233, 'distribute': 4234, 'planned': 4235, 'tangle': 4236, 'hill': 4237, 'fractures': 4238, 'queens': 4239, 'constructive': 4240, 'facto': 4241, 'prosecutor': 4242, 'application': 4243, 'trying': 4244, 'divided': 4245, 'september-october': 4246, 'bexar': 4247, 'atlantic': 4248, \"don't\": 4249, 'disable': 4250, 'resources': 4251, 'destroy': 4252, '300': 4253, 'manner': 4254, 'fighters': 4255, 'stop': 4256, 'fort': 4257, 'parade': 4258, 'sites': 4259, 'grove': 4260, 'severe': 4261, 'protests': 4262, 'flows': 4263, 'disagreement': 4264, 'hoped': 4265, 'acclaimed': 4266, 'residential': 4267, '1959': 4268, 'tract': 4269, 'alloted': 4270, 'some': 4271}\n"
     ]
    }
   ],
   "source": [
    "#numericalization\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n"
     ]
    }
   ],
   "source": [
    "#vocab size\n",
    "voc_size = len(vocab)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append UNK\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just in case we need to use\n",
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Co-occurence Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# index the corpus\n",
    "X_i = Counter(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "# Prepare the skipgram\n",
    "\n",
    "for doc in corpus:\n",
    "    # The skipgram has a window size of 2\n",
    "    for i in range(2, len(doc)-2):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-1], doc[i+1],doc[i+2],doc[i-2]]\n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgrams = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check whether the co-occurences between w_i and w_j is available\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "        #if not exist, then set to 1 \"laplace smoothing\"\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #set xmax\n",
    "    x_max = 100\n",
    "    #set alpha\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    #otherwise, set to 1\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  # for keeping the co-occurrences\n",
    "weighting_dic = {}  # scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgrams.get(bigram) is not None:  # matches\n",
    "        co_occer = X_ik_skipgrams[bigram]  # get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1  # + 1 for stability issue\n",
    "        X_ik[(bigram[1], bigram[0])] = co_occer + 1  # count also for the opposite\n",
    "        # print(X_ik[(bigram[1], bigram[0])])  # count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(2, len(doc)-2):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 4 words\n",
    "            outside = (word2index[doc[i-1]], word2index[doc[i+1]], word2index[doc[i+2]], word2index[doc[i-2]])\n",
    "            #for each of these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                \n",
    "                #center, outside1;   center, outside2\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1477],\n",
       "       [2734]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4273"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(63314, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.LongTensor(x)\n",
    "embedding(x_tensor).shape  #(batch_size, 1, emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Skipgram with positive sampling\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Skipgram with negative sampling\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(\\mathbf{v}_c,o,\\mathbf{U})=-\\log(\\sigma(\\mathbf{u}_o^T\\mathbf{v}_c))-\\sum_{k=1}^K\\log(\\sigma(-\\mathbf{u}_k^T\\mathbf{v}_c))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, emb_size) # center embedding\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, emb_size) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds    = -self.embedding_outside(negative_words) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
    "                \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.embedding_center(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# setting the dataset\n",
    "glove_file = datapath('D:/AIT/Sem2/NLP/NLP_Assignments/glove.6B.100d.txt')\n",
    "model_gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    2,  ..., 4270, 4271,    0],\n",
       "        [   0,    1,    2,  ..., 4270, 4271,    0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocab of batch - 2 , vocab - 2 and embed - 2\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocab)\n",
    "emb_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(4273, 2)\n",
       "  (embedding_outside): Embedding(4273, 2)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_skipgram_positive = Skipgram(voc_size, emb_size)\n",
    "model_skipgram_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glove(\n",
       "  (embedding_center): Embedding(4273, 2)\n",
       "  (embedding_outside): Embedding(4273, 2)\n",
       "  (center_bias): Embedding(4273, 1)\n",
       "  (outside_bias): Embedding(4273, 1)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove = Glove(voc_size, emb_size)\n",
    "model_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(4273, 2)\n",
       "  (embedding_outside): Embedding(4273, 2)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_skipgram_negative = Skipgram(voc_size, emb_size)\n",
    "model_skipgram_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_skipgram_positive = model_skipgram_positive(input_tensor, label_tensor, all_vocabs)\n",
    "loss_skipgram_negative = model_skipgram_negative(input_tensor, label_tensor, all_vocabs)\n",
    "# x, y, cooc, weighting = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "# loss_glove = model_glove(torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(cooc), torch.LongTensor(weighting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model_skipgram_positive      = Skipgram(voc_size, emb_size)\n",
    "optimizer_skipgram_positive  = optim.Adam(model_skipgram_positive.parameters(), lr=0.001)\n",
    "optimizer_skipgram_negative  = optim.Adam(model_skipgram_negative.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Skigram\n",
      "Epoch      1 | Loss: 9.853922| time: 0m 1s\n",
      "Positive Skigram\n",
      "Epoch      2 | Loss: 9.974030| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      3 | Loss: 8.819837| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      4 | Loss: 8.312509| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      5 | Loss: 8.443431| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      6 | Loss: 8.337053| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      7 | Loss: 9.400013| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      8 | Loss: 8.098328| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      9 | Loss: 13.149128| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch     10 | Loss: 12.942814| time: 0m 0s\n",
      "Total runtime: 3.44 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss_skipgram_positive = model_skipgram_positive(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_skipgram_positive.zero_grad()\n",
    "    loss_skipgram_positive.backward()\n",
    "\n",
    "    #update alpha\n",
    "    optimizer_skipgram_positive.step()\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss_skipgram_positive\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(\"Positive Skigram\")\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram_positive:2.6f}| time: {epoch_mins}m {epoch_secs}s\")\n",
    "# Record the ending time\n",
    "total_end = time.time()\n",
    "\n",
    "# Calculate and print the total runtime\n",
    "total_runtime = total_end - total_start\n",
    "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Skigram\n",
      "Epoch      1 | Loss: 8.488328 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      2 | Loss: 11.110193 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      3 | Loss: 8.415419 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      4 | Loss: 9.590928 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      5 | Loss: 9.421183 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      6 | Loss: 8.911397 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      7 | Loss: 10.771469 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      8 | Loss: 7.993955 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      9 | Loss: 9.968192 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch     10 | Loss: 8.916197 | time: 0m 0s\n",
      "Total runtime: 1.65 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss_skipgram_negative = model_skipgram_negative(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_skipgram_negative.zero_grad()\n",
    "    loss_skipgram_negative.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_skipgram_negative.step()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss_skipgram_positive\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(\"Negative Skigram\")\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram_negative:2.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "# Record the ending time\n",
    "total_end = time.time()\n",
    "\n",
    "# Calculate and print the total runtime\n",
    "total_runtime = total_end - total_start\n",
    "print(f\"Total runtime: {total_runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove\n",
      "Epoch      1 | Loss: 0.605260 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      2 | Loss: 0.231935 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      3 | Loss: 2.960654 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      4 | Loss: 0.556752 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      5 | Loss: 0.342571 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      6 | Loss: 0.494570 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      7 | Loss: 0.419004 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      8 | Loss: 5.999505 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      9 | Loss: 45.718384 | time: 0m 0s\n",
      "Glove\n",
      "Epoch     10 | Loss: 0.328714 | time: 0m 0s\n",
      "Total runtime: 1.65 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch)\n",
    "    \n",
    "    #predict   \n",
    "    loss_glove = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss_glove.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_glove.step()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss_skipgram_positive\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(\"Glove\")\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_glove:2.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "total_runtime = total_end - total_start\n",
    "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Window Size  | Training Loss | Training Time (sec) |\n",
    "|------------------|-------------|---------------|---------------------|\n",
    "| Skipgram         |      2      |    9.4        |     2.6             |\n",
    "| Skipgram (NEG)   |      2      |    9.3981     |     1.23            |\n",
    "| Glove            |      2      |    0.8        |     1.23            |\n",
    "| Glove (Gensim)   |      -      |    -          |      -              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(model, word):\n",
    "    try:\n",
    "        # Find the index\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        # if not found give the index of unknown token\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    # get the word in terms of tensor\n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "     # embed the center and the outside word and then find the final embed\n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_embed_for_corpus(model, words):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except KeyError:\n",
    "            index = word2index['<UNK>']\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "\n",
    "        embed_c = model.embedding_center(word_tensor)\n",
    "        embed_o = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_c + embed_o) / 2\n",
    "\n",
    "        # return as dictionary with key as the word and value as the array of its embedding\n",
    "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_for_corpus(embeddings, target_word):\n",
    "    # List to store (word, cosine_similarity) pairs\n",
    "    similarities = []\n",
    "\n",
    "    # Get the index of the target word or use the index for '<UNK>' if not found\n",
    "    target_index = word2index.get(target_word, word2index['<UNK>'])\n",
    "    \n",
    "    # Get the vector for the target word\n",
    "    target_vector = embeddings[target_index]\n",
    "\n",
    "    # Iterate through all words in the embeddings dictionary\n",
    "    for word, vector in embeddings.items():\n",
    "        # Calculate the cosine similarity between the target word and the current word\n",
    "        similarity = cosine_similarity(target_vector, vector)\n",
    "        \n",
    "        # Append the (word, cosine_similarity) pair to the list\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the predicted y using the different models\n",
    "### Using Word analogies dataset  \n",
    "Dataset taken from [website](https://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your .txt file\n",
    "file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    # Skip the first line\n",
    "    file.readline()\n",
    "\n",
    "    # Read the remaining content of the file\n",
    "    file_content = file.readlines()\n",
    "\n",
    "# Initialize variables to store relevant lines\n",
    "total_corpus = []\n",
    "\n",
    "# Variable to keep track of the current heading\n",
    "current_heading = None\n",
    "\n",
    "# Iterate through each line in the file content\n",
    "for line in file_content:\n",
    "    # Check if the line is a heading\n",
    "    if line.startswith(':'):\n",
    "        current_heading = line.strip()\n",
    "    else:\n",
    "        # Split the line into individual words and convert to lowercase\n",
    "        words = [word.lower() for word in line.strip().split()]\n",
    "        total_corpus.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.readlines()\n",
    "\n",
    "# Initialize variables to store relevant lines\n",
    "capital_common_countries = []\n",
    "past_tense = []\n",
    "\n",
    "# Variable to keep track of the current heading\n",
    "current_heading = None\n",
    "\n",
    "# Iterate through each line in the file content\n",
    "for line in file_content:\n",
    "    # Check if the line is a heading\n",
    "    if line.startswith(':'):\n",
    "        current_heading = line.strip()\n",
    "    elif current_heading == ': capital-common-countries':\n",
    "        # Split the line into individual words and convert to lowercase\n",
    "        words = [word.lower() for word in line.strip().split()]\n",
    "        capital_common_countries.append(words)\n",
    "    elif current_heading == ': gram7-past-tense':\n",
    "        # Split the line into individual words and convert to lowercase\n",
    "        words = [word.lower() for word in line.strip().split()]\n",
    "        past_tense.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 2D list into a list of lists\n",
    "flattened_list_of_country = [word for pair in capital_common_countries for word in pair]\n",
    "\n",
    "# Wrap the flattened list in another list\n",
    "resulting_capital_list = [flattened_list_of_country]\n",
    "\n",
    "# Flatten the 2D list into a list of lists\n",
    "flattened_list_of_past_tense = [word for pair in past_tense for word in pair]\n",
    "\n",
    "# Wrap the flattened list in another list\n",
    "resulting_capital_list = [flattened_list_of_country]\n",
    "resulting_past_tense_list = [flattened_list_of_past_tense]\n",
    "\n",
    "# Flatten the 2D list into a list of lists\n",
    "flattened_list_total_words = [word for pair in total_corpus for word in pair]\n",
    "# Wrap the flattened list in another list\n",
    "resulting_total_corpus = [flattened_list_total_words]\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "capital_list = list(set(flatten(resulting_capital_list)))\n",
    "past_tense_list = list(set(flatten(resulting_past_tense_list)))\n",
    "whole_corpus = list(set(flatten(resulting_total_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embeddings\n",
    "embed_capital_glove = get_embed_for_corpus(model_glove, capital_list)\n",
    "embed_capital_skipgram_positive = get_embed_for_corpus(model_skipgram_positive, capital_list)\n",
    "embed_capital_skipgram_negative = get_embed_for_corpus(model_skipgram_negative, capital_list)\n",
    "\n",
    "embed_past_tense_glove = get_embed_for_corpus(model_glove, past_tense_list)\n",
    "embed_past_tense_skipgram_positive = get_embed_for_corpus(model_skipgram_positive, past_tense_list)\n",
    "embed_past_tense_skipgram_negative = get_embed_for_corpus(model_skipgram_negative, past_tense_list)\n",
    "\n",
    "embed_total_glove = get_embed_for_corpus(model_glove, whole_corpus)\n",
    "embed_whole_skipgram_positive = get_embed_for_corpus(model_skipgram_positive, whole_corpus)\n",
    "embed_whole_skipgram_negative = get_embed_for_corpus(model_skipgram_negative, whole_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for glove for the capital list\n",
    "y_pred_glove_country = []\n",
    "\n",
    "for i in capital_common_countries:  \n",
    "    y = embed_capital_glove[i[1]] - embed_capital_glove[i[0]] + embed_capital_glove[i[2]]\n",
    "    y_pred_glove_country.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for glove for the past tense list\n",
    "y_pred_glove_past = []\n",
    "\n",
    "for i in past_tense:  \n",
    "    y = embed_past_tense_glove[i[1]] - embed_past_tense_glove[i[0]] + embed_past_tense_glove[i[2]]\n",
    "    y_pred_glove_past.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram negative sampling for the capital list\n",
    "y_pred_neg_samp_country = []\n",
    "\n",
    "for i in capital_common_countries:  \n",
    "    y = embed_capital_skipgram_negative[i[1]] - embed_capital_skipgram_negative[i[0]] + embed_capital_skipgram_negative[i[2]]\n",
    "    y_pred_neg_samp_country.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram negative sampling for the past tense list\n",
    "y_pred_neg_samp_past = []\n",
    "\n",
    "for i in past_tense:  \n",
    "    y = embed_past_tense_skipgram_negative[i[0]] - embed_past_tense_skipgram_negative[i[0]] + embed_past_tense_skipgram_negative[i[2]]\n",
    "    y_pred_neg_samp_past.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram positive sampling for the country list\n",
    "y_pred_positive_samp_country = []\n",
    "\n",
    "for i in capital_common_countries: \n",
    "    y = embed_capital_skipgram_positive[i[1]] - embed_capital_skipgram_positive[i[0]] + embed_capital_skipgram_positive[i[2]]\n",
    "    y_pred_positive_samp_country.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram positive sampling for the past tense list\n",
    "y_pred_positive_past_tense = []\n",
    "\n",
    "for i in past_tense: \n",
    "    y = embed_past_tense_skipgram_positive[i[1]] - embed_past_tense_skipgram_positive[i[0]] + embed_past_tense_skipgram_positive[i[2]]\n",
    "    y_pred_positive_past_tense.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the cosine similarity\n",
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_cosine_words(y_pred, embeddings):\n",
    "    \"\"\"\n",
    "    Find the word with the maximum cosine similarity for each vector in y_pred.\n",
    "\n",
    "    Parameters:\n",
    "    - y_pred: List of vectors for which to find the max cosine similarity words.\n",
    "    - embeddings: Dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - List of words with the maximum cosine similarity for each vector in y_pred.\n",
    "    \"\"\"\n",
    "    max_cosine_words = []\n",
    "\n",
    "    for j in range(len(y_pred)):\n",
    "        max_cosine = -1\n",
    "        max_cosine_word = \"\"\n",
    "\n",
    "        for i in embeddings.keys():\n",
    "            cosine_temp = cosine_similarity(y_pred[j], embeddings[i])\n",
    "\n",
    "            if cosine_temp > max_cosine:\n",
    "                max_cosine_word = i\n",
    "                max_cosine = cosine_temp\n",
    "\n",
    "        max_cosine_words.append(max_cosine_word)\n",
    "\n",
    "    return max_cosine_words\n",
    "\n",
    "#Usage\n",
    "cosine_neg_samp_syntatical = find_max_cosine_words(y_pred_neg_samp_country, embed_capital_skipgram_negative)\n",
    "cosine_positive_samp_syntatical = find_max_cosine_words(y_pred_positive_samp_country, embed_capital_skipgram_positive)\n",
    "cosine_glove_syntatical = find_max_cosine_words(y_pred_glove_country, embed_capital_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 10 similar words for user-provided word 'greece': ['samoa', 'go', 'georgia', 'deeper', 'quiet', 'see', 'women', 'worcester', 'arlington', 'afghanistan']\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "def find_next_10_cosine_words_for_word(target_word, embeddings, top_n=10):\n",
    "    \"\"\"\n",
    "    Find the next 10 words with the maximum cosine similarity for a user-provided specific word.\n",
    "\n",
    "    Parameters:\n",
    "    - target_word: The word for which to find the next 10 cosine similarity words.\n",
    "    - embeddings: Dictionary of word embeddings.\n",
    "    - top_n: Number of top words to retrieve for the target word (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - List of the next 10 words with the maximum cosine similarity for the target word or [\"Word not in Corpus\"].\n",
    "    \"\"\"\n",
    "    if target_word not in embeddings:\n",
    "        return [\"Word not in Corpus\"]\n",
    "\n",
    "    target_vector = embeddings[target_word]\n",
    "    cosine_similarities = [(word, cosine_similarity(target_vector, embeddings[word])) for word in embeddings.keys()]\n",
    "    top_n_words = nlargest(top_n + 1, cosine_similarities, key=lambda x: x[1])\n",
    "\n",
    "    # Exclude the target word itself\n",
    "    top_n_words = [word for word, _ in top_n_words if word != target_word]\n",
    "\n",
    "    return top_n_words[:10]\n",
    "\n",
    "# Usage:\n",
    "user_target_word = 'greece'\n",
    "next_10_cosine_for_user_word = find_next_10_cosine_words_for_word(user_target_word, embed_whole_skipgram_negative, top_n=10)\n",
    "\n",
    "# Print the results\n",
    "if next_10_cosine_for_user_word == [\"Word not in Corpus\"]:\n",
    "    print(\"Word not in Corpus\")\n",
    "else:\n",
    "    print(f\"Next 10 similar words for user-provided word '{user_target_word}': {next_10_cosine_for_user_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Semantic Accuracy\n",
    "(athens/greece/kathmandu --> Nepal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy of Skipgram Negative: 14.4268774704%\n",
      "Semantic Accuracy of Skipgram Positive: 14.2292490119%\n",
      "Semantic Accuracy of Glove: 14.4268774704%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, true_words):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on predictions and true words.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: List of predicted words.\n",
    "    - true_words: List of true words.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    total_trials = len(predictions)\n",
    "    total_correct = sum(1 for pred_word in predictions if pred_word in true_words)\n",
    "\n",
    "    accuracy = (total_correct / total_trials) * 100\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Usage:\n",
    "semantic_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_country, embed_whole_skipgram_negative), [true_word[3] for true_word in capital_common_countries])\n",
    "semantic_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_samp_country, embed_whole_skipgram_positive), [true_word[3] for true_word in capital_common_countries])\n",
    "semantic_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_country, embed_total_glove), [true_word[3] for true_word in capital_common_countries])\n",
    "print(\"Semantic Accuracy of Skipgram Negative: {:.10f}%\".format(semantic_accuracy_neg_samp))\n",
    "print(\"Semantic Accuracy of Skipgram Positive: {:.10f}%\".format(semantic_accuracy_pos_samp))\n",
    "print(\"Semantic Accuracy of Glove: {:.10f}%\".format(semantic_accuracy_glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line removed and content saved to: D:/AIT/Sem2/NLP/NLP_Assignments/word-test-without-first-line.txt\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "output_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test-without-first-line.txt'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read all lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Write all lines except the first line to the output file\n",
    "    output_file.writelines(lines[1:])\n",
    "\n",
    "print(f\"First line removed and content saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines starting with ': capital-countries' saved to: D:/AIT/Sem2/NLP/NLP_Assignments/capital.txt\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "output_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/capital.txt'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read all lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Flag to indicate whether to start writing lines\n",
    "    start_writing = False\n",
    "\n",
    "    # Iterate through lines\n",
    "    for line in lines:\n",
    "        # Check if the line starts with ': gram7-past-tense'\n",
    "        if line.startswith(': capital-common-countries'):\n",
    "            # Set the flag to start writing\n",
    "            start_writing = True\n",
    "        elif line.startswith(':'):\n",
    "            # If a new section header is encountered, stop writing\n",
    "            start_writing = False\n",
    "\n",
    "        # Write lines to the output file if the flag is True\n",
    "        if start_writing:\n",
    "            output_file.write(line)\n",
    "\n",
    "print(f\"Lines starting with ': capital-countries' saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semtatical Accuracy of Model Gensim: 0.9387351778656127\n"
     ]
    }
   ],
   "source": [
    "analogy_score_sem = model_gensim.evaluate_word_analogies(datapath('D:/AIT/Sem2/NLP/NLP_Assignments/capital.txt'))\n",
    "print(\"Semtatical Accuracy of Model Gensim:\", analogy_score_sem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Syntatical Accuracy\n",
    "run, runs, walk --> walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntatical Accuracy of Skipgram Negative: 0.00%\n",
      "Syntatical Accuracy of Skipgram Positive: 11.41%\n",
      "Syntatical Accuracy of Glove: 13.27%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, true_words):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on predictions and true words.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: List of predicted words.\n",
    "    - true_words: List of true words.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    total_trials = len(predictions)\n",
    "    total_correct = sum(1 for pred_word in predictions if pred_word in true_words)\n",
    "\n",
    "    accuracy = (total_correct / total_trials) * 100\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Usage:\n",
    "syntatical_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_past, embed_whole_skipgram_negative), [true_word[3] for true_word in past_tense])\n",
    "syntatical_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_past_tense, embed_whole_skipgram_positive), [true_word[3] for true_word in past_tense])\n",
    "syntatical_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_past, embed_total_glove), [true_word[3] for true_word in past_tense])\n",
    "print(\"Syntatical Accuracy of Skipgram Negative: {:.2f}%\".format(syntatical_accuracy_neg_samp))\n",
    "print(\"Syntatical Accuracy of Skipgram Positive: {:.2f}%\".format(syntatical_accuracy_pos_samp))\n",
    "print(\"Syntatical Accuracy of Glove: {:.2f}%\".format(syntatical_accuracy_glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines starting with ': gram7-past-tense' saved to: D:/AIT/Sem2/NLP/NLP_Assignments/past_tense_lines.txt\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "output_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/past_tense_lines.txt'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read all lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Flag to indicate whether to start writing lines\n",
    "    start_writing = False\n",
    "\n",
    "    # Iterate through lines\n",
    "    for line in lines:\n",
    "        # Check if the line starts with ': gram7-past-tense'\n",
    "        if line.startswith(': gram7-past-tense'):\n",
    "            # Set the flag to start writing\n",
    "            start_writing = True\n",
    "        elif line.startswith(':'):\n",
    "            # If a new section header is encountered, stop writing\n",
    "            start_writing = False\n",
    "\n",
    "        # Write lines to the output file if the flag is True\n",
    "        if start_writing:\n",
    "            output_file.write(line)\n",
    "\n",
    "print(f\"Lines starting with ': gram7-past-tense' saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntatical Accuracy of Model Gensim: 0.5544871794871795\n"
     ]
    }
   ],
   "source": [
    "analogy_score_syn = model_gensim.evaluate_word_analogies(datapath('D:/AIT/Sem2/NLP/NLP_Assignments/past_tense_lines.txt'))\n",
    "print(\"Syntatical Accuracy of Model Gensim:\", analogy_score_syn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision between models\n",
    "\n",
    "\n",
    "| Model            | Window Size | Training Loss | Training Time(sec) | Syntactic Accuracy | Semantic Accuracy |\n",
    "|------------------|-------------|---------------|--------------------|--------------------|-------------------|\n",
    "| Skipgram         |      2      |      8.68     |      2.6           |        14.23%      |     16.35%        |\n",
    "| Skipgram (NEG)   |      2      |      8.56     |      1.23          |        14.23%      |     0.00%         |\n",
    "| Glove            |      2      |      0.8      |      1.23          |        14.82%      |     10.83%        |\n",
    "| Glove (Gensim)   |      -      |       -       |       -            |         93.8%      |     0.55%         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spearman Coefficient\n",
    "Dataset taken from  [website](http://alfonseca.org/eng/research/wordsim353.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarity_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarity_index\n",
       "0         tiger       cat              7.35\n",
       "1         tiger     tiger             10.00\n",
       "2         plane       car              5.77\n",
       "3         train       car              6.31\n",
       "4    television     radio              6.77\n",
       "..          ...       ...               ...\n",
       "198     rooster    voyage              0.62\n",
       "199        noon    string              0.54\n",
       "200       chord     smile              0.54\n",
       "201   professor  cucumber              0.31\n",
       "202        king   cabbage              0.23\n",
       "\n",
       "[203 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt'\n",
    "\n",
    "# Define the column names\n",
    "columns = ['word_1', 'word_2', 'similarity_index']\n",
    "\n",
    "# Read the text file into a pandas DataFrame with specified column names\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=columns)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.21927808225154877, 0.5070983171463013)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed(model_skipgram_negative,'<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarity_index</th>\n",
       "      <th>dot_product_neg_samp</th>\n",
       "      <th>dot_product_pos_samp</th>\n",
       "      <th>dot_product_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>-0.265244</td>\n",
       "      <td>-0.084458</td>\n",
       "      <td>-0.622820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarity_index  dot_product_neg_samp  \\\n",
       "0         tiger       cat              7.35              0.305232   \n",
       "1         tiger     tiger             10.00              0.305232   \n",
       "2         plane       car              5.77              0.305232   \n",
       "3         train       car              6.31              0.305232   \n",
       "4    television     radio              6.77             -0.265244   \n",
       "..          ...       ...               ...                   ...   \n",
       "198     rooster    voyage              0.62              0.305232   \n",
       "199        noon    string              0.54              0.305232   \n",
       "200       chord     smile              0.54              0.305232   \n",
       "201   professor  cucumber              0.31              0.305232   \n",
       "202        king   cabbage              0.23              0.305232   \n",
       "\n",
       "     dot_product_pos_samp  dot_product_glove  \n",
       "0                2.073911           1.849394  \n",
       "1                2.073911           1.849394  \n",
       "2                2.073911           1.849394  \n",
       "3                2.073911           1.849394  \n",
       "4               -0.084458          -0.622820  \n",
       "..                    ...                ...  \n",
       "198              2.073911           1.849394  \n",
       "199              2.073911           1.849394  \n",
       "200              2.073911           1.849394  \n",
       "201              2.073911           1.849394  \n",
       "202              2.073911           1.849394  \n",
       "\n",
       "[203 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    word_1 = row['word_1']\n",
    "    word_2 = row['word_2']\n",
    "\n",
    "    try:\n",
    "        # Attempt to get embeddings and compute the dot product\n",
    "        embed_1_neg_samp = get_embed(model_skipgram_negative, word_1)\n",
    "        embed_2_neg_samp = get_embed(model_skipgram_negative, word_2)\n",
    "        embed_1_pos_samp = get_embed(model_skipgram_positive, word_1)\n",
    "        embed_2_pos_samp = get_embed(model_skipgram_positive, word_2)\n",
    "        embed_1_glove = get_embed(model_glove, word_1)\n",
    "        embed_2_glove = get_embed(model_glove, word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Handle the case where one or both words are not present in the model\n",
    "        # Replace missing embeddings with the embedding of '<UNK>' or any other suitable value\n",
    "        embed_1_neg_samp = get_embed(model_skipgram_negative, '<UNK>')\n",
    "        embed_2_neg_samp = get_embed(model_skipgram_negative, '<UNK>')\n",
    "        embed_1_pos_samp = get_embed(model_skipgram_positive, '<UNK>')\n",
    "        embed_2_pos_samp = get_embed(model_skipgram_positive, '<UNK>')\n",
    "        embed_1_glove = get_embed(model_glove, '<UNK>')\n",
    "        embed_2_glove = get_embed(model_glove, '<UNK>')\n",
    "\n",
    "    # Compute the dot product and update the DataFrame\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
    "    df.at[index, 'dot_product_pos_samp'] = np.dot(embed_1_pos_samp, embed_2_pos_samp)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Coefficient of Skipgram Negative: 0.0475\n",
      "Spearman Correlation Coefficient of Skipgram Positive: 0.0658\n",
      "Spearman Correlation Coefficient of Glove: -0.0124\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute the Spearman correlation between the provided similarity scores and your models' dot products\n",
    "correlation_neg, _ = spearmanr(df['similarity_index'], df['dot_product_neg_samp'])\n",
    "correlation_pos, _ = spearmanr(df['similarity_index'], df['dot_product_pos_samp'])\n",
    "correlation_glove, _ = spearmanr(df['similarity_index'], df['dot_product_glove'])\n",
    "\n",
    "\n",
    "# Display the correlation coefficient\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram Negative: {correlation_neg:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram Positive: {correlation_pos:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 5.13\n"
     ]
    }
   ],
   "source": [
    "# Finding y_true based on the mean of similarity index in the df\n",
    "y_true = df['similarity_index'].mean()\n",
    "\n",
    "print(f\"y_true: {y_true:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Using the correlation coeffiecient of the gensim model using the predefined function\n",
    "correlation_coefficient = model_gensim.evaluate_word_pairs(datapath('D:/AIT/Sem2/NLP/NLP_Assignments/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt'))\n",
    "print(f\"Correlation coefficient: {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Skipgram (POS) | Skipgram (NEG)   | GloVe   | GloVe (gensim) | Y true     |\n",
    "|-----------------|----------------|------------------|---------|----------------|------------|\n",
    "| MSE             |     0.0325     |     0.0297       | 0.0494  | 0.60           | 5.13       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim model saved to: model/model_gensim.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming you have a Gensim Word2Vec model named 'model'\n",
    "# You can replace 'Word2Vec' with the specific Gensim model you are using\n",
    "\n",
    "# Save the Gensim model to a file using pickle\n",
    "gensim_model_path = 'model/model_gensim.pkl'\n",
    "\n",
    "with open(gensim_model_path, 'wb') as model_file:\n",
    "    pickle.dump(model_gensim, model_file)\n",
    "\n",
    "print(f\"Gensim model saved to: {gensim_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your pickled Gensim model file\n",
    "gensim_model_path = 'model/model_gensim.pkl'\n",
    "\n",
    "# Load the Gensim model from the pickle file\n",
    "with open(gensim_model_path, 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "spoken\n",
      "arabic\n",
      "english\n",
      "dialect\n",
      "vocabulary\n",
      "text\n",
      "translation\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,10):\n",
    "    print(loaded_model.most_similar('language')[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary saved to: model/embed_skipgram_positive.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming embed_capital_skipgram_negative is your embedding dictionary\n",
    "embedding_dict = embed_whole_skipgram_positive\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = 'model/embed_skipgram_positive.pkl'\n",
    "\n",
    "# Open the file in binary write mode and dump the dictionary\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(embedding_dict, pickle_file)\n",
    "\n",
    "print(f\"Embedding dictionary saved to: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary saved to: model/embed_skipgram_negative.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming embed_capital_skipgram_negative is your embedding dictionary\n",
    "embedding_dict = embed_whole_skipgram_negative\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = 'model/embed_skipgram_negative.pkl'\n",
    "\n",
    "# Open the file in binary write mode and dump the dictionary\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(embedding_dict, pickle_file)\n",
    "\n",
    "print(f\"Embedding dictionary saved to: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary saved to: model/embed_glove.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming embed_capital_skipgram_negative is your embedding dictionary\n",
    "embedding_dict = embed_total_glove\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = 'model/embed_glove.pkl'\n",
    "\n",
    "# Open the file in binary write mode and dump the dictionary\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(embedding_dict, pickle_file)\n",
    "\n",
    "print(f\"Embedding dictionary saved to: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickled file on the server\n",
    "pickle_file_path = 'model/embed_skipgram_positive.pkl'\n",
    "\n",
    "# Load the embedding dictionary from the pickled file\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    embedding_dict_neg = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickled file on the server\n",
    "pickle_file_path = 'model/embed_skipgram_negative.pkl'\n",
    "\n",
    "# Load the embedding dictionary from the pickled file\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    embedding_dict_pos = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickled file on the server\n",
    "pickle_file_path = 'model/embed_glove.pkl'\n",
    "\n",
    "# Load the embedding dictionary from the pickled file\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    embedding_dict_glove = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 10 similar words for user-provided word 'thailand': ['man', 'described', 'looked', 'older', 'looking', 'dollar', 'says', 'said', 'reading', 'flying']\n"
     ]
    }
   ],
   "source": [
    "user_target_word = \"thailand\"\n",
    "next_10_cosine_for_user_word = find_next_10_cosine_words_for_word(user_target_word, embedding_dict_glove, top_n=10)\n",
    "\n",
    "# Print the results\n",
    "if next_10_cosine_for_user_word == [\"Word not in Corpus\"]:\n",
    "    print(\"Word not in Corpus\")\n",
    "else:\n",
    "    print(f\"Next 10 similar words for user-provided word '{user_target_word}': {next_10_cosine_for_user_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The window size, length of corpus plays a very important hand in determining the accuracy of a model.\n",
    "\n",
    "The models from scratch (Skipgram (positive sampling, negative sampling), Glove) performed quite badly when compared to the Gensim model as they are trained with a low corpus size (1000) in a low epoch time of 10 and in a reduced window size of 2. \n",
    "\n",
    "Similarly, increasing the corpus size lead to issues in the glove model while finding the co-occurence size.\n",
    "\n",
    "The training loss of Glove is significantly lower in comparision to Skipgram and Skipgram (NEG) suggesting that the Glove has learned efficiently during the training which might be due to the fact it is a simple model.\n",
    "\n",
    "While Skipgram (NEG) has the shortest training time along with glove, which could be attributed to the negative sampling technique used in training.\n",
    "\n",
    "Whereas Skipgram Positive presented itself with better Semantic accuracy which help classify semantic identification as a task of positive sampling. \n",
    "\n",
    "Furthermore, the spearmans's correlation (which measures the monotonic relationship between two variables. A higher absolute value indicates a stronger monotonic relationship) shows GloVe (Gensim) has the highest absolute Spearman's correlation coefficient (0.0494), indicating a relatively stronger monotonic relationship between its predictions and the true values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
