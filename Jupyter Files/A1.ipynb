{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\LEGION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.4', '2.1.2+cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Create a corpus containing only documents from the 'earn' category\n",
    "corpus = brown.sents()\n",
    "\n",
    "# Limit the corpus to the first 1000 sentences for demonstration purposes\n",
    "corpus = [[word.lower() for word in sentence] for sentence in corpus]\n",
    "corpus = corpus[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word sequences and unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4272"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalization\n",
    "word2index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n"
     ]
    }
   ],
   "source": [
    "#vocab size\n",
    "voc_size = len(vocab)\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append UNK\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just in case we need to use\n",
    "index2word = {v:k for k, v in word2index.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Co-occurence Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# index the corpus\n",
    "X_i = Counter(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "# Prepare the skipgram\n",
    "\n",
    "for doc in corpus:\n",
    "    # The skipgram has a window size of 2\n",
    "    for i in range(2, len(doc)-2):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-1], doc[i+1],doc[i+2],doc[i-2]]\n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgrams = Counter(skip_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check whether the co-occurences between w_i and w_j is available\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "        #if not exist, then set to 1 \"laplace smoothing\"\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #set xmax\n",
    "    x_max = 100\n",
    "    #set alpha\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    #otherwise, set to 1\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {}  # for keeping the co-occurrences\n",
    "weighting_dic = {}  # scaling the percentage of sampling\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    if X_ik_skipgrams.get(bigram) is not None:  # matches\n",
    "        co_occer = X_ik_skipgrams[bigram]  # get the count from what we already counted\n",
    "        X_ik[bigram] = co_occer + 1  # + 1 for stability issue\n",
    "        X_ik[(bigram[1], bigram[0])] = co_occer + 1  # count also for the opposite\n",
    "        # print(X_ik[(bigram[1], bigram[0])])  # count also for the opposite\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(2, len(doc)-2):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 4 words\n",
    "            outside = (word2index[doc[i-1]], word2index[doc[i+1]], word2index[doc[i+2]], word2index[doc[i-2]])\n",
    "            #for each of these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                \n",
    "                #center, outside1;   center, outside2\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #convert to id since our skip_grams is word, not yet id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 268],\n",
       "       [3682]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4273"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(63314, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.LongTensor(x)\n",
    "embedding(x_tensor).shape  #(batch_size, 1, emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Skipgram with positive sampling\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Skipgram with negative sampling\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(\\mathbf{v}_c,o,\\mathbf{U})=-\\log(\\sigma(\\mathbf{u}_o^T\\mathbf{v}_c))-\\sum_{k=1}^K\\log(\\sigma(-\\mathbf{u}_k^T\\mathbf{v}_c))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, emb_size) # center embedding\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, emb_size) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_center(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_outside(target_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds    = -self.embedding_outside(negative_words) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
    "                \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.embedding_center(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# setting the dataset\n",
    "glove_file = datapath('D:/AIT/Sem2/NLP/NLP_Assignments/glove.6B.100d.txt')\n",
    "model_gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    2,  ..., 4270, 4271,    0],\n",
       "        [   0,    1,    2,  ..., 4270, 4271,    0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocab of batch - 2 , vocab - 2 and embed - 2\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocab)\n",
    "emb_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(4273, 2)\n",
       "  (embedding_outside): Embedding(4273, 2)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_skipgram_positive = Skipgram(voc_size, emb_size)\n",
    "model_skipgram_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glove(\n",
       "  (embedding_center): Embedding(4273, 2)\n",
       "  (embedding_outside): Embedding(4273, 2)\n",
       "  (center_bias): Embedding(4273, 1)\n",
       "  (outside_bias): Embedding(4273, 1)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove = Glove(voc_size, emb_size)\n",
    "model_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(4273, 2)\n",
       "  (embedding_outside): Embedding(4273, 2)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_skipgram_negative = Skipgram(voc_size, emb_size)\n",
    "model_skipgram_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_skipgram_positive = model_skipgram_positive(input_tensor, label_tensor, all_vocabs)\n",
    "loss_skipgram_negative = model_skipgram_negative(input_tensor, label_tensor, all_vocabs)\n",
    "# x, y, cooc, weighting = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "# loss_glove = model_glove(torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(cooc), torch.LongTensor(weighting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model_skipgram_positive      = Skipgram(voc_size, emb_size)\n",
    "optimizer_skipgram_positive  = optim.Adam(model_skipgram_positive.parameters(), lr=0.001)\n",
    "optimizer_skipgram_negative  = optim.Adam(model_skipgram_negative.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Skigram\n",
      "Epoch      1 | Loss: 9.450300| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      2 | Loss: 10.412741| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      3 | Loss: 7.812970| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      4 | Loss: 10.702510| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      5 | Loss: 8.898797| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      6 | Loss: 9.200410| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      7 | Loss: 8.337086| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      8 | Loss: 10.390896| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch      9 | Loss: 9.237535| time: 0m 0s\n",
      "Positive Skigram\n",
      "Epoch     10 | Loss: 12.773212| time: 0m 0s\n",
      "Total runtime: 1.87 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss_skipgram_positive = model_skipgram_positive(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_skipgram_positive.zero_grad()\n",
    "    loss_skipgram_positive.backward()\n",
    "\n",
    "    #update alpha\n",
    "    optimizer_skipgram_positive.step()\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss_skipgram_positive\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(\"Positive Skigram\")\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram_positive:2.6f}| time: {epoch_mins}m {epoch_secs}s\")\n",
    "# Record the ending time\n",
    "total_end = time.time()\n",
    "\n",
    "# Calculate and print the total runtime\n",
    "total_runtime = total_end - total_start\n",
    "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Skigram\n",
      "Epoch      1 | Loss: 9.800464 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      2 | Loss: 7.853164 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      3 | Loss: 9.314707 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      4 | Loss: 7.424520 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      5 | Loss: 9.622099 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      6 | Loss: 8.495985 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      7 | Loss: 9.595748 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      8 | Loss: 8.537954 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch      9 | Loss: 8.436984 | time: 0m 0s\n",
      "Negative Skigram\n",
      "Epoch     10 | Loss: 8.785051 | time: 0m 0s\n",
      "Total runtime: 1.81 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss_skipgram_negative = model_skipgram_negative(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_skipgram_negative.zero_grad()\n",
    "    loss_skipgram_negative.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_skipgram_negative.step()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss_skipgram_positive\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(\"Negative Skigram\")\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_skipgram_negative:2.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "# Record the ending time\n",
    "total_end = time.time()\n",
    "\n",
    "# Calculate and print the total runtime\n",
    "total_runtime = total_end - total_start\n",
    "print(f\"Total runtime: {total_runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove\n",
      "Epoch      1 | Loss: 1.703771 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      2 | Loss: 0.171729 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      3 | Loss: 28.697340 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      4 | Loss: 0.024538 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      5 | Loss: 0.586223 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      6 | Loss: 0.025614 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      7 | Loss: 25.135370 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      8 | Loss: 1.015011 | time: 0m 0s\n",
      "Glove\n",
      "Epoch      9 | Loss: 3.492422 | time: 0m 0s\n",
      "Glove\n",
      "Epoch     10 | Loss: 0.226656 | time: 0m 0s\n",
      "Total runtime: 1.81 seconds\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch)\n",
    "    \n",
    "    #predict   \n",
    "    loss_glove = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss_glove.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_glove.step()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    #print the loss_skipgram_positive\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(\"Glove\")\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_glove:2.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "total_runtime = total_end - total_start\n",
    "print(f\"Total runtime: {total_runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Window Size  | Training Loss | Training Time (sec) |\n",
    "|------------------|-------------|---------------|---------------------|\n",
    "| Skipgram         |      2      |    9.4        |     2.6             |\n",
    "| Skipgram (NEG)   |      2      |    9.3981     |     1.23            |\n",
    "| Glove            |      2      |    0.8        |     1.23            |\n",
    "| Glove (Gensim)   |      -      |    -          |      -              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(model, word):\n",
    "    try:\n",
    "        # Find the index\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        # if not found give the index of unknown token\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    # get the word in terms of tensor\n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "     # embed the center and the outside word and then find the final embed\n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_embed_for_corpus(model, words):\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except KeyError:\n",
    "            index = word2index['<UNK>']\n",
    "\n",
    "        word_tensor = torch.LongTensor([index])\n",
    "\n",
    "        embed_c = model.embedding_center(word_tensor)\n",
    "        embed_o = model.embedding_outside(word_tensor)\n",
    "        embed = (embed_c + embed_o) / 2\n",
    "\n",
    "        # return as dictionary with key as the word and value as the array of its embedding\n",
    "        embeddings[word] = np.array([embed[0][0].item(), embed[0][1].item()])\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_for_corpus(embeddings, target_word):\n",
    "    # List to store (word, cosine_similarity) pairs\n",
    "    similarities = []\n",
    "\n",
    "    # Get the index of the target word or use the index for '<UNK>' if not found\n",
    "    target_index = word2index.get(target_word, word2index['<UNK>'])\n",
    "    \n",
    "    # Get the vector for the target word\n",
    "    target_vector = embeddings[target_index]\n",
    "\n",
    "    # Iterate through all words in the embeddings dictionary\n",
    "    for word, vector in embeddings.items():\n",
    "        # Calculate the cosine similarity between the target word and the current word\n",
    "        similarity = cosine_similarity(target_vector, vector)\n",
    "        \n",
    "        # Append the (word, cosine_similarity) pair to the list\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the predicted y using the different models\n",
    "### Using Word analogies dataset  \n",
    "Dataset taken from [website](https://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your .txt file\n",
    "file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    # Skip the first line\n",
    "    file.readline()\n",
    "\n",
    "    # Read the remaining content of the file\n",
    "    file_content = file.readlines()\n",
    "\n",
    "# Initialize variables to store relevant lines\n",
    "total_corpus = []\n",
    "\n",
    "# Variable to keep track of the current heading\n",
    "current_heading = None\n",
    "\n",
    "# Iterate through each line in the file content\n",
    "for line in file_content:\n",
    "    # Check if the line is a heading\n",
    "    if line.startswith(':'):\n",
    "        current_heading = line.strip()\n",
    "    else:\n",
    "        # Split the line into individual words and convert to lowercase\n",
    "        words = [word.lower() for word in line.strip().split()]\n",
    "        total_corpus.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.readlines()\n",
    "\n",
    "# Initialize variables to store relevant lines\n",
    "capital_common_countries = []\n",
    "past_tense = []\n",
    "\n",
    "# Variable to keep track of the current heading\n",
    "current_heading = None\n",
    "\n",
    "# Iterate through each line in the file content\n",
    "for line in file_content:\n",
    "    # Check if the line is a heading\n",
    "    if line.startswith(':'):\n",
    "        current_heading = line.strip()\n",
    "    elif current_heading == ': capital-common-countries':\n",
    "        # Split the line into individual words and convert to lowercase\n",
    "        words = [word.lower() for word in line.strip().split()]\n",
    "        capital_common_countries.append(words)\n",
    "    elif current_heading == ': gram7-past-tense':\n",
    "        # Split the line into individual words and convert to lowercase\n",
    "        words = [word.lower() for word in line.strip().split()]\n",
    "        past_tense.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 2D list into a list of lists\n",
    "flattened_list_of_country = [word for pair in capital_common_countries for word in pair]\n",
    "\n",
    "# Wrap the flattened list in another list\n",
    "resulting_capital_list = [flattened_list_of_country]\n",
    "\n",
    "# Flatten the 2D list into a list of lists\n",
    "flattened_list_of_past_tense = [word for pair in past_tense for word in pair]\n",
    "\n",
    "# Wrap the flattened list in another list\n",
    "resulting_capital_list = [flattened_list_of_country]\n",
    "resulting_past_tense_list = [flattened_list_of_past_tense]\n",
    "\n",
    "# Flatten the 2D list into a list of lists\n",
    "flattened_list_total_words = [word for pair in total_corpus for word in pair]\n",
    "# Wrap the flattened list in another list\n",
    "resulting_total_corpus = [flattened_list_total_words]\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "capital_list = list(set(flatten(resulting_capital_list)))\n",
    "past_tense_list = list(set(flatten(resulting_past_tense_list)))\n",
    "whole_corpus = list(set(flatten(resulting_total_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embeddings\n",
    "embed_capital_glove = get_embed_for_corpus(model_glove, capital_list)\n",
    "embed_capital_skipgram_positive = get_embed_for_corpus(model_skipgram_positive, capital_list)\n",
    "embed_capital_skipgram_negative = get_embed_for_corpus(model_skipgram_negative, capital_list)\n",
    "\n",
    "embed_past_tense_glove = get_embed_for_corpus(model_glove, past_tense_list)\n",
    "embed_past_tense_skipgram_positive = get_embed_for_corpus(model_skipgram_positive, past_tense_list)\n",
    "embed_past_tense_skipgram_negative = get_embed_for_corpus(model_skipgram_negative, past_tense_list)\n",
    "\n",
    "embed_total_glove = get_embed_for_corpus(model_glove, whole_corpus)\n",
    "embed_whole_skipgram_positive = get_embed_for_corpus(model_skipgram_positive, whole_corpus)\n",
    "embed_whole_skipgram_negative = get_embed_for_corpus(model_skipgram_negative, whole_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for glove for the capital list\n",
    "y_pred_glove_country = []\n",
    "\n",
    "for i in capital_common_countries:  \n",
    "    y = embed_capital_glove[i[1]] - embed_capital_glove[i[0]] + embed_capital_glove[i[2]]\n",
    "    y_pred_glove_country.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for glove for the past tense list\n",
    "y_pred_glove_past = []\n",
    "\n",
    "for i in past_tense:  \n",
    "    y = embed_past_tense_glove[i[1]] - embed_past_tense_glove[i[0]] + embed_past_tense_glove[i[2]]\n",
    "    y_pred_glove_past.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram negative sampling for the capital list\n",
    "y_pred_neg_samp_country = []\n",
    "\n",
    "for i in capital_common_countries:  \n",
    "    y = embed_capital_skipgram_negative[i[1]] - embed_capital_skipgram_negative[i[0]] + embed_capital_skipgram_negative[i[2]]\n",
    "    y_pred_neg_samp_country.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram negative sampling for the past tense list\n",
    "y_pred_neg_samp_past = []\n",
    "\n",
    "for i in past_tense:  \n",
    "    y = embed_past_tense_skipgram_negative[i[0]] - embed_past_tense_skipgram_negative[i[0]] + embed_past_tense_skipgram_negative[i[2]]\n",
    "    y_pred_neg_samp_past.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram positive sampling for the country list\n",
    "y_pred_positive_samp_country = []\n",
    "\n",
    "for i in capital_common_countries: \n",
    "    y = embed_capital_skipgram_positive[i[1]] - embed_capital_skipgram_positive[i[0]] + embed_capital_skipgram_positive[i[2]]\n",
    "    y_pred_positive_samp_country.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred for skipgram positive sampling for the past tense list\n",
    "y_pred_positive_past_tense = []\n",
    "\n",
    "for i in past_tense: \n",
    "    y = embed_past_tense_skipgram_positive[i[1]] - embed_past_tense_skipgram_positive[i[0]] + embed_past_tense_skipgram_positive[i[2]]\n",
    "    y_pred_positive_past_tense.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the cosine similarity\n",
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_cosine_words(y_pred, embeddings):\n",
    "    \"\"\"\n",
    "    Find the word with the maximum cosine similarity for each vector in y_pred.\n",
    "\n",
    "    Parameters:\n",
    "    - y_pred: List of vectors for which to find the max cosine similarity words.\n",
    "    - embeddings: Dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - List of words with the maximum cosine similarity for each vector in y_pred.\n",
    "    \"\"\"\n",
    "    max_cosine_words = []\n",
    "\n",
    "    for j in range(len(y_pred)):\n",
    "        max_cosine = -1\n",
    "        max_cosine_word = \"\"\n",
    "\n",
    "        for i in embeddings.keys():\n",
    "            cosine_temp = cosine_similarity(y_pred[j], embeddings[i])\n",
    "\n",
    "            if cosine_temp > max_cosine:\n",
    "                max_cosine_word = i\n",
    "                max_cosine = cosine_temp\n",
    "\n",
    "        max_cosine_words.append(max_cosine_word)\n",
    "\n",
    "    return max_cosine_words\n",
    "\n",
    "#Usage\n",
    "cosine_neg_samp_syntatical = find_max_cosine_words(y_pred_neg_samp_country, embed_capital_skipgram_negative)\n",
    "cosine_positive_samp_syntatical = find_max_cosine_words(y_pred_positive_samp_country, embed_capital_skipgram_positive)\n",
    "cosine_glove_syntatical = find_max_cosine_words(y_pred_glove_country, embed_capital_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 10 similar words for user-provided word 'greece': ['old', 'go', 'free', 'stronger', 'increased', 'child', 'samoa', 'older', 'looking', 'big']\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "def find_next_10_cosine_words_for_word(target_word, embeddings, top_n=10):\n",
    "    \"\"\"\n",
    "    Find the next 10 words with the maximum cosine similarity for a user-provided specific word.\n",
    "\n",
    "    Parameters:\n",
    "    - target_word: The word for which to find the next 10 cosine similarity words.\n",
    "    - embeddings: Dictionary of word embeddings.\n",
    "    - top_n: Number of top words to retrieve for the target word (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - List of the next 10 words with the maximum cosine similarity for the target word or [\"Word not in Corpus\"].\n",
    "    \"\"\"\n",
    "    if target_word not in embeddings:\n",
    "        return [\"Word not in Corpus\"]\n",
    "\n",
    "    target_vector = embeddings[target_word]\n",
    "    cosine_similarities = [(word, cosine_similarity(target_vector, embeddings[word])) for word in embeddings.keys()]\n",
    "    top_n_words = nlargest(top_n + 1, cosine_similarities, key=lambda x: x[1])\n",
    "\n",
    "    # Exclude the target word itself\n",
    "    top_n_words = [word for word, _ in top_n_words if word != target_word]\n",
    "\n",
    "    return top_n_words[:10]\n",
    "\n",
    "# Usage:\n",
    "user_target_word = 'greece'\n",
    "next_10_cosine_for_user_word = find_next_10_cosine_words_for_word(user_target_word, embed_whole_skipgram_negative, top_n=10)\n",
    "\n",
    "# Print the results\n",
    "if next_10_cosine_for_user_word == [\"Word not in Corpus\"]:\n",
    "    print(\"Word not in Corpus\")\n",
    "else:\n",
    "    print(f\"Next 10 similar words for user-provided word '{user_target_word}': {next_10_cosine_for_user_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Semantic Accuracy\n",
    "(athens/greece/kathmandu --> Nepal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accuracy of Skipgram Negative: 14.6245059289%\n",
      "Semantic Accuracy of Skipgram Positive: 14.4268774704%\n",
      "Semantic Accuracy of Glove: 18.3794466403%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, true_words):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on predictions and true words.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: List of predicted words.\n",
    "    - true_words: List of true words.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    total_trials = len(predictions)\n",
    "    total_correct = sum(1 for pred_word in predictions if pred_word in true_words)\n",
    "\n",
    "    accuracy = (total_correct / total_trials) * 100\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Usage:\n",
    "semantic_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_country, embed_whole_skipgram_negative), [true_word[3] for true_word in capital_common_countries])\n",
    "semantic_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_samp_country, embed_whole_skipgram_positive), [true_word[3] for true_word in capital_common_countries])\n",
    "semantic_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_country, embed_total_glove), [true_word[3] for true_word in capital_common_countries])\n",
    "print(\"Semantic Accuracy of Skipgram Negative: {:.10f}%\".format(semantic_accuracy_neg_samp))\n",
    "print(\"Semantic Accuracy of Skipgram Positive: {:.10f}%\".format(semantic_accuracy_pos_samp))\n",
    "print(\"Semantic Accuracy of Glove: {:.10f}%\".format(semantic_accuracy_glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line removed and content saved to: D:/AIT/Sem2/NLP/NLP_Assignments/word-test-without-first-line.txt\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "output_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test-without-first-line.txt'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read all lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Write all lines except the first line to the output file\n",
    "    output_file.writelines(lines[1:])\n",
    "\n",
    "print(f\"First line removed and content saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines starting with ': capital-countries' saved to: D:/AIT/Sem2/NLP/NLP_Assignments/capital.txt\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "output_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/capital.txt'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read all lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Flag to indicate whether to start writing lines\n",
    "    start_writing = False\n",
    "\n",
    "    # Iterate through lines\n",
    "    for line in lines:\n",
    "        # Check if the line starts with ': gram7-past-tense'\n",
    "        if line.startswith(': capital-common-countries'):\n",
    "            # Set the flag to start writing\n",
    "            start_writing = True\n",
    "        elif line.startswith(':'):\n",
    "            # If a new section header is encountered, stop writing\n",
    "            start_writing = False\n",
    "\n",
    "        # Write lines to the output file if the flag is True\n",
    "        if start_writing:\n",
    "            output_file.write(line)\n",
    "\n",
    "print(f\"Lines starting with ': capital-countries' saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semtatical Accuracy of Model Gensim: 0.9387351778656127\n"
     ]
    }
   ],
   "source": [
    "analogy_score_sem = model_gensim.evaluate_word_analogies(datapath('D:/AIT/Sem2/NLP/NLP_Assignments/capital.txt'))\n",
    "print(\"Semtatical Accuracy of Model Gensim:\", analogy_score_sem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Syntatical Accuracy\n",
    "run, runs, walk --> walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntatical Accuracy of Skipgram Negative: 0.00%\n",
      "Syntatical Accuracy of Skipgram Positive: 15.19%\n",
      "Syntatical Accuracy of Glove: 10.90%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, true_words):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on predictions and true words.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: List of predicted words.\n",
    "    - true_words: List of true words.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    total_trials = len(predictions)\n",
    "    total_correct = sum(1 for pred_word in predictions if pred_word in true_words)\n",
    "\n",
    "    accuracy = (total_correct / total_trials) * 100\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Usage:\n",
    "syntatical_accuracy_neg_samp = calculate_accuracy(find_max_cosine_words(y_pred_neg_samp_past, embed_whole_skipgram_negative), [true_word[3] for true_word in past_tense])\n",
    "syntatical_accuracy_pos_samp = calculate_accuracy(find_max_cosine_words(y_pred_positive_past_tense, embed_whole_skipgram_positive), [true_word[3] for true_word in past_tense])\n",
    "syntatical_accuracy_glove = calculate_accuracy(find_max_cosine_words(y_pred_glove_past, embed_total_glove), [true_word[3] for true_word in past_tense])\n",
    "print(\"Syntatical Accuracy of Skipgram Negative: {:.2f}%\".format(syntatical_accuracy_neg_samp))\n",
    "print(\"Syntatical Accuracy of Skipgram Positive: {:.2f}%\".format(syntatical_accuracy_pos_samp))\n",
    "print(\"Syntatical Accuracy of Glove: {:.2f}%\".format(syntatical_accuracy_glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines starting with ': gram7-past-tense' saved to: D:/AIT/Sem2/NLP/NLP_Assignments/past_tense_lines.txt\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/word-test.v1.txt'\n",
    "output_file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/past_tense_lines.txt'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read all lines from the input file\n",
    "    lines = input_file.readlines()\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    # Flag to indicate whether to start writing lines\n",
    "    start_writing = False\n",
    "\n",
    "    # Iterate through lines\n",
    "    for line in lines:\n",
    "        # Check if the line starts with ': gram7-past-tense'\n",
    "        if line.startswith(': gram7-past-tense'):\n",
    "            # Set the flag to start writing\n",
    "            start_writing = True\n",
    "        elif line.startswith(':'):\n",
    "            # If a new section header is encountered, stop writing\n",
    "            start_writing = False\n",
    "\n",
    "        # Write lines to the output file if the flag is True\n",
    "        if start_writing:\n",
    "            output_file.write(line)\n",
    "\n",
    "print(f\"Lines starting with ': gram7-past-tense' saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntatical Accuracy of Model Gensim: 0.5544871794871795\n"
     ]
    }
   ],
   "source": [
    "analogy_score_syn = model_gensim.evaluate_word_analogies(datapath('D:/AIT/Sem2/NLP/NLP_Assignments/past_tense_lines.txt'))\n",
    "print(\"Syntatical Accuracy of Model Gensim:\", analogy_score_syn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision between models\n",
    "\n",
    "\n",
    "| Model            | Window Size | Training Loss | Training Time(sec) | Syntactic Accuracy | Semantic Accuracy |\n",
    "|------------------|-------------|---------------|--------------------|--------------------|-------------------|\n",
    "| Skipgram         |      2      |      8.68     |      2.6           |        14.23%      |     16.35%        |\n",
    "| Skipgram (NEG)   |      2      |      8.56     |      1.23          |        14.23%      |     0.00%         |\n",
    "| Glove            |      2      |      0.8      |      1.23          |        14.82%      |     10.83%        |\n",
    "| Glove (Gensim)   |      -      |       -       |       -            |         93.8%      |     55%           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spearman Coefficient\n",
    "Dataset taken from  [website](http://alfonseca.org/eng/research/wordsim353.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarity_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarity_index\n",
       "0         tiger       cat              7.35\n",
       "1         tiger     tiger             10.00\n",
       "2         plane       car              5.77\n",
       "3         train       car              6.31\n",
       "4    television     radio              6.77\n",
       "..          ...       ...               ...\n",
       "198     rooster    voyage              0.62\n",
       "199        noon    string              0.54\n",
       "200       chord     smile              0.54\n",
       "201   professor  cucumber              0.31\n",
       "202        king   cabbage              0.23\n",
       "\n",
       "[203 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'D:/AIT/Sem2/NLP/NLP_Assignments/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt'\n",
    "\n",
    "# Define the column names\n",
    "columns = ['word_1', 'word_2', 'similarity_index']\n",
    "\n",
    "# Read the text file into a pandas DataFrame with specified column names\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=columns)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0373988151550293, 0.3067092299461365)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed(model_skipgram_negative,'<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarity_index</th>\n",
       "      <th>dot_product_neg_samp</th>\n",
       "      <th>dot_product_pos_samp</th>\n",
       "      <th>dot_product_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>-0.265244</td>\n",
       "      <td>-0.084458</td>\n",
       "      <td>-0.622820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>media</td>\n",
       "      <td>radio</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bread</td>\n",
       "      <td>butter</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cucumber</td>\n",
       "      <td>potato</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.305232</td>\n",
       "      <td>2.073911</td>\n",
       "      <td>1.849394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doctor</td>\n",
       "      <td>nurse</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.220851</td>\n",
       "      <td>-0.120827</td>\n",
       "      <td>1.074796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>professor</td>\n",
       "      <td>doctor</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>-0.102049</td>\n",
       "      <td>-0.490062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_1  word_2  similarity_index  dot_product_neg_samp  \\\n",
       "0       tiger     cat              7.35              0.305232   \n",
       "1       tiger   tiger             10.00              0.305232   \n",
       "2       plane     car              5.77              0.305232   \n",
       "3       train     car              6.31              0.305232   \n",
       "4  television   radio              6.77             -0.265244   \n",
       "5       media   radio              7.42              0.305232   \n",
       "6       bread  butter              6.19              0.305232   \n",
       "7    cucumber  potato              5.92              0.305232   \n",
       "8      doctor   nurse              7.00              0.220851   \n",
       "9   professor  doctor              6.62              0.021121   \n",
       "\n",
       "   dot_product_pos_samp  dot_product_glove  \n",
       "0              2.073911           1.849394  \n",
       "1              2.073911           1.849394  \n",
       "2              2.073911           1.849394  \n",
       "3              2.073911           1.849394  \n",
       "4             -0.084458          -0.622820  \n",
       "5              2.073911           1.849394  \n",
       "6              2.073911           1.849394  \n",
       "7              2.073911           1.849394  \n",
       "8             -0.120827           1.074796  \n",
       "9             -0.102049          -0.490062  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    word_1 = row['word_1']\n",
    "    word_2 = row['word_2']\n",
    "\n",
    "    try:\n",
    "        # Attempt to get embeddings and compute the dot product\n",
    "        embed_1_neg_samp = get_embed(model_skipgram_negative, word_1)\n",
    "        embed_2_neg_samp = get_embed(model_skipgram_negative, word_2)\n",
    "        embed_1_pos_samp = get_embed(model_skipgram_positive, word_1)\n",
    "        embed_2_pos_samp = get_embed(model_skipgram_positive, word_2)\n",
    "        embed_1_glove = get_embed(model_glove, word_1)\n",
    "        embed_2_glove = get_embed(model_glove, word_2)\n",
    "\n",
    "    except KeyError:\n",
    "        # Handle the case where one or both words are not present in the model\n",
    "        # Replace missing embeddings with the embedding of '<UNK>' or any other suitable value\n",
    "        embed_1_neg_samp = get_embed(model_skipgram_negative, '<UNK>')\n",
    "        embed_2_neg_samp = get_embed(model_skipgram_negative, '<UNK>')\n",
    "        embed_1_pos_samp = get_embed(model_skipgram_positive, '<UNK>')\n",
    "        embed_2_pos_samp = get_embed(model_skipgram_positive, '<UNK>')\n",
    "        embed_1_glove = get_embed(model_glove, '<UNK>')\n",
    "        embed_2_glove = get_embed(model_glove, '<UNK>')\n",
    "\n",
    "    # Compute the dot product and update the DataFrame\n",
    "    df.at[index, 'dot_product_neg_samp'] = np.dot(embed_1_neg_samp, embed_2_neg_samp)\n",
    "    df.at[index, 'dot_product_pos_samp'] = np.dot(embed_1_pos_samp, embed_2_pos_samp)\n",
    "    df.at[index, 'dot_product_glove'] = np.dot(embed_1_glove, embed_2_glove)\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation Coefficient of Skipgram Negative: 0.0429\n",
      "Spearman Correlation Coefficient of Skipgram Positive: 0.0253\n",
      "Spearman Correlation Coefficient of Glove: -0.1017\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute the Spearman correlation between the provided similarity scores and your models' dot products\n",
    "correlation_neg, _ = spearmanr(df['similarity_index'], df['dot_product_neg_samp'])\n",
    "correlation_pos, _ = spearmanr(df['similarity_index'], df['dot_product_pos_samp'])\n",
    "correlation_glove, _ = spearmanr(df['similarity_index'], df['dot_product_glove'])\n",
    "\n",
    "\n",
    "# Display the correlation coefficient\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram Negative: {correlation_neg:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Skipgram Positive: {correlation_pos:.4f}\")\n",
    "print(f\"Spearman Correlation Coefficient of Glove: {correlation_glove:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 5.13\n"
     ]
    }
   ],
   "source": [
    "# Finding y_true based on the mean of similarity index in the df\n",
    "y_true = df['similarity_index'].mean()\n",
    "\n",
    "print(f\"y_true: {y_true:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Using the correlation coeffiecient of the gensim model using the predefined function\n",
    "correlation_coefficient = model_gensim.evaluate_word_pairs(datapath('D:/AIT/Sem2/NLP/NLP_Assignments/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt'))\n",
    "print(f\"Correlation coefficient: {correlation_coefficient[1][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Skipgram (POS) | Skipgram (NEG)   | GloVe   | GloVe (gensim) | Y true     |\n",
    "|-----------------|----------------|------------------|---------|----------------|------------|\n",
    "| MSE             |     0.0325     |     0.0297       | 0.0494  | 0.60           | 5.13       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_whole_glove = get_embed_for_corpus(model_glove, vocab)\n",
    "embed_whole_neg_skg = get_embed_for_corpus(model_skipgram_negative, vocab)\n",
    "embed_whole_pos_skg = get_embed_for_corpus(model_skipgram_positive, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim model saved to: model/model_gensim.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming you have a Gensim Word2Vec model named 'model'\n",
    "# You can replace 'Word2Vec' with the specific Gensim model you are using\n",
    "\n",
    "# Save the Gensim model to a file using pickle\n",
    "gensim_model_path = 'model/model_gensim.pkl'\n",
    "\n",
    "with open(gensim_model_path, 'wb') as model_file:\n",
    "    pickle.dump(model_gensim, model_file)\n",
    "\n",
    "print(f\"Gensim model saved to: {gensim_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your pickled Gensim model file\n",
    "gensim_model_path = 'model/model_gensim.pkl'\n",
    "\n",
    "# Load the Gensim model from the pickle file\n",
    "with open(gensim_model_path, 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "spoken\n",
      "arabic\n",
      "english\n",
      "dialect\n",
      "vocabulary\n",
      "text\n",
      "translation\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,10):\n",
    "    print(loaded_model.most_similar('language')[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary saved to: model/embed_skipgram_positive.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming embed_capital_skipgram_negative is your embedding dictionary\n",
    "embedding_dict = embed_whole_pos_skg\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = 'model/embed_skipgram_positive.pkl'\n",
    "\n",
    "# Open the file in binary write mode and dump the dictionary\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(embedding_dict, pickle_file)\n",
    "\n",
    "print(f\"Embedding dictionary saved to: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary saved to: model/embed_skipgram_negative.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming embed_capital_skipgram_negative is your embedding dictionary\n",
    "embedding_dict = embed_whole_neg_skg\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = 'model/embed_skipgram_negative.pkl'\n",
    "\n",
    "# Open the file in binary write mode and dump the dictionary\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(embedding_dict, pickle_file)\n",
    "\n",
    "print(f\"Embedding dictionary saved to: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary saved to: model/embed_glove.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming embed_capital_skipgram_negative is your embedding dictionary\n",
    "embedding_dict = embed_whole_glove\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = 'model/embed_glove.pkl'\n",
    "\n",
    "# Open the file in binary write mode and dump the dictionary\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(embedding_dict, pickle_file)\n",
    "\n",
    "print(f\"Embedding dictionary saved to: {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickled file on the server\n",
    "pickle_file_path = 'model/embed_skipgram_positive.pkl'\n",
    "\n",
    "# Load the embedding dictionary from the pickled file\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    embedding_dict_neg = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickled file on the server\n",
    "pickle_file_path = 'model/embed_skipgram_negative.pkl'\n",
    "\n",
    "# Load the embedding dictionary from the pickled file\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    embedding_dict_pos = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickled file on the server\n",
    "pickle_file_path = 'model/embed_glove.pkl'\n",
    "\n",
    "# Load the embedding dictionary from the pickled file\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    embedding_dict_glove = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 10 similar words for user-provided word 'run': ['salary', 'foster', 'peace', '$740,000', 'known', 'spend', 'will', 'acute', 'legitimate', 'manner']\n"
     ]
    }
   ],
   "source": [
    "user_target_word = \"run\"\n",
    "next_10_cosine_for_user_word = find_next_10_cosine_words_for_word(user_target_word, embedding_dict_glove, top_n=10)\n",
    "\n",
    "# Print the results\n",
    "if next_10_cosine_for_user_word == [\"Word not in Corpus\"]:\n",
    "    print(\"Word not in Corpus\")\n",
    "else:\n",
    "    print(f\"Next 10 similar words for user-provided word '{user_target_word}': {next_10_cosine_for_user_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The window size, length of corpus plays a very important hand in determining the accuracy of a model.\n",
    "\n",
    "The models from scratch (Skipgram (positive sampling, negative sampling), Glove) performed quite badly when compared to the Gensim model as they are trained with a low corpus size (1000) in a low epoch time of 10 and in a reduced window size of 2. \n",
    "\n",
    "Similarly, increasing the corpus size lead to issues in the glove model while finding the co-occurence size.\n",
    "\n",
    "The training loss of Glove is significantly lower in comparision to Skipgram and Skipgram (NEG) suggesting that the Glove has learned efficiently during the training which might be due to the fact it is a simple model.\n",
    "\n",
    "While Skipgram (NEG) has the shortest training time along with glove, which could be attributed to the negative sampling technique used in training.\n",
    "\n",
    "Whereas Skipgram Positive presented itself with better Semantic accuracy which help classify semantic identification as a task of positive sampling. \n",
    "\n",
    "Furthermore, the spearmans's correlation (which measures the monotonic relationship between two variables. A higher absolute value indicates a stronger monotonic relationship) shows GloVe (Gensim) has the highest absolute Spearman's correlation coefficient (0.0494), indicating a relatively stronger monotonic relationship between its predictions and the true values.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
