{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks and Language Models\n",
    "\n",
    "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
    "\n",
    "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Text File\n",
    "\n",
    "Here is a text files of all Harry Potter books taken from https://www.kaggle.com/code/shubhammisar/harry-potter-character-word2vec-embedding (Kaggle).  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a collection of all the magical world of Harry Potter in a text file. The text is already pretty clean, there are no extra spaces or weird characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 67785\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from datasets import load_dataset_builder, Dataset\n",
    "\n",
    "# Define the path to your custom dataset file\n",
    "custom_dataset_path = \"..\\datasets\\Harry_Potter_all_books_preprocessed.txt\"\n",
    "\n",
    "# Read data from the custom dataset file\n",
    "with open(custom_dataset_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Split the data into sentences based on the \".\" delimiter and create a list of dictionaries\n",
    "data = data.split(\" .\")\n",
    "data = [{\"text\": row} for row in data]\n",
    "\n",
    "# Create a Dataset object from the list of dictionaries\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 54228\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6779\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "train_test_valid = train_test['test'].train_test_split(test_size=0.5)\n",
    "# gathering all into a single datasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test_valid['test'],\n",
    "    'validation': train_test_valid['train']})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is difficult sir\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you try to change the index you might notice that sometimes there is no paragraph \\nand rather an empty string so we will have to care of that later.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train']['text'][33])\n",
    "\n",
    "'''\n",
    "If you try to change the index you might notice that sometimes there is no paragraph \n",
    "and rather an empty string so we will have to care of that later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14439c014d64ccabaad6fca8f95c9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc06bbf3280e4c5f87d326227ac669eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c847e203d494690b512f40e4453cb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'me', 'third', 'year']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "#function to tokenize\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "\n",
    "#map the function to each example\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][333]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11063\n",
      "['<unk>', '<eos>', 'the', 'and', 'to', 'of', 'a', 'he', 'harry', 'was']\n"
     ]
    }
   ],
   "source": [
    "## numericalizing\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
    "min_freq=3) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)            \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11063"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Save the vocabulary using pickle\n",
    "with open('model/vocab_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the vocabulary from the saved file\n",
    "with open('model/vocab_lm.pkl', 'rb') as f:\n",
    "    loaded_vocab = pickle.load(f)\n",
    "\n",
    "# Now, loaded_vocab contains the vocabulary loaded from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Preprocessing\n",
    "\n",
    "Firstly, the text data is read from a custom dataset file. Then, the data is split into individual sentences using the delimiter \" .\", and each sentence is converted into a dictionary format with the key \"text\". These dictionaries are used to create a dataset using the Dataset class from the datasets library.\n",
    "\n",
    "The dataset is further divided into training and testing sets using the train_test_split method, with a test size of 0.2. The resulting split is stored in the train_test variable. The testing set is then split again into testing and validation sets using the train_test_split method, with a test size of 0.5. This split is stored in the train_test_valid variable.\n",
    "\n",
    "To tokenize the dataset, the TorchText library's get_tokenizer function is employed to obtain an English language tokenizer. A tokenize_data function is defined to tokenize each example in the dataset using the tokenizer. The map method is utilized to apply the tokenize_data function to each example, removing the original 'text' column and adding a new 'tokens' column containing the tokenized version of the text.\n",
    "\n",
    "Next, the vocabulary is built using the build_vocab_from_iterator method from the TorchText library. The vocabulary is constructed from the tokenized examples in the 'train' split, considering only tokens that appear at least three times. Special tokens like '<unk>' (unknown) and '<eos>' (end of sentence) are inserted into the vocabulary, with the default index set to the index of the '<unk>' token.\n",
    "\n",
    "Finally, the length of the vocabulary and the first 10 tokens are printed to verify the vocabulary construction. These preprocessing steps form a fundamental pipeline for text data, including tokenization and vocabulary building, which are essential for various natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    # Initialize an empty list to store tokenized and numericalized data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through examples in the dataset\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            # Append '<eos>' token to mark the end of a sequence\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            \n",
    "            # Numericalize tokens using the vocabulary\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            \n",
    "            # Extend the data list with the numericalized tokens\n",
    "            data.extend(tokens)\n",
    "\n",
    "    # Convert the data list to a PyTorch LongTensor\n",
    "    data = torch.LongTensor(data)\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "\n",
    "    # Make the data batch evenly by discarding any remaining tokens\n",
    "    data = data[:num_batches * batch_size]\n",
    "\n",
    "    # Reshape the data into [batch_size, num_batches] tensor\n",
    "    data = data.view(batch_size, num_batches)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        # Constructor for the LSTM Language Model\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize model parameters\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Layers of the model\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                            dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize weights for embedding and fully connected layers\n",
    "        \n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        \n",
    "        # Initialize embedding weights\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        \n",
    "        # Initialize fully connected layer weights\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "        # Initialize LSTM weights\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                                                            self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                                                            self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden and cell states for LSTM\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        # Detach hidden and cell states for backpropagation through time (BPTT)\n",
    "        \n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        # Forward pass through the model\n",
    "        \n",
    "        # src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        # embedding: [batch size, seq len, emb_dim]\n",
    "        \n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        # output: [batch size, seq len, hid_dim]\n",
    "        # hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        # prediction: [batch size, seq_len, vocab size]\n",
    "        \n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The LSTMLanguageModel architecture is a powerful model for language modeling tasks. It consists of an embedding layer, LSTM layers, a dropout layer, and a linear layer. The embedding layer learns dense representations for input tokens, capturing their semantic relationships. The LSTM layers process sequences, capturing long-term dependencies and updating hidden and cell states. Stacking multiple LSTM layers enables the model to learn more complex patterns. The dropout layer prevents overfitting by randomly zeroing out inputs during training. The linear layer maps hidden states to the vocabulary size, producing predicted probabilities for each token. This architecture allows the model to understand and generate coherent sequences. It is beneficial for tasks such as next-word prediction, text generation, machine translation, and sentiment analysis. The LSTMLanguageModel architecture's combination of embedding, LSTM, dropout, and linear layers provides a robust framework for capturing and generating sequential patterns in language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 39,461,687 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the LSTMLanguageModel\n",
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "\n",
    "# Initialize the optimizer with Adam and model parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define the loss criterion for training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the total number of trainable parameters in the model\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print(f'The model has {num_params:,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the vocabulary from the saved file\n",
    "with open('../Jupyter Files//model/vocab_lm.pkl', 'rb') as f:\n",
    "    loaded_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11063"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 642.301\n",
      "\tValid Perplexity: 462.123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 334.065\n",
      "\tValid Perplexity: 229.390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 234.021\n",
      "\tValid Perplexity: 184.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 195.799\n",
      "\tValid Perplexity: 162.273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 172.806\n",
      "\tValid Perplexity: 149.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 157.209\n",
      "\tValid Perplexity: 139.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 145.488\n",
      "\tValid Perplexity: 132.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 135.854\n",
      "\tValid Perplexity: 127.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 128.282\n",
      "\tValid Perplexity: 123.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 121.705\n",
      "\tValid Perplexity: 120.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 115.885\n",
      "\tValid Perplexity: 117.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 111.209\n",
      "\tValid Perplexity: 115.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 106.588\n",
      "\tValid Perplexity: 113.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 102.572\n",
      "\tValid Perplexity: 112.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 98.934\n",
      "\tValid Perplexity: 110.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 95.733\n",
      "\tValid Perplexity: 109.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 92.790\n",
      "\tValid Perplexity: 109.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 89.863\n",
      "\tValid Perplexity: 108.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 87.477\n",
      "\tValid Perplexity: 108.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 85.085\n",
      "\tValid Perplexity: 108.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 82.843\n",
      "\tValid Perplexity: 107.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 80.674\n",
      "\tValid Perplexity: 106.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 78.866\n",
      "\tValid Perplexity: 106.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 77.093\n",
      "\tValid Perplexity: 106.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 73.877\n",
      "\tValid Perplexity: 106.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 72.239\n",
      "\tValid Perplexity: 106.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 70.603\n",
      "\tValid Perplexity: 105.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 69.703\n",
      "\tValid Perplexity: 105.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 69.169\n",
      "\tValid Perplexity: 105.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 68.165\n",
      "\tValid Perplexity: 105.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 67.707\n",
      "\tValid Perplexity: 105.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 67.308\n",
      "\tValid Perplexity: 105.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.920\n",
      "\tValid Perplexity: 105.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.846\n",
      "\tValid Perplexity: 105.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.736\n",
      "\tValid Perplexity: 105.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.692\n",
      "\tValid Perplexity: 105.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.660\n",
      "\tValid Perplexity: 105.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.592\n",
      "\tValid Perplexity: 105.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.591\n",
      "\tValid Perplexity: 105.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 66.646\n",
      "\tValid Perplexity: 105.688\n"
     ]
    }
   ],
   "source": [
    "# Set the number of training epochs\n",
    "n_epochs = 40\n",
    "\n",
    "# Set the decoding length for sequence generation\n",
    "seq_len = 50  # <----decoding length\n",
    "\n",
    "# Set the gradient clipping threshold\n",
    "clip = 0.25\n",
    "\n",
    "# Initialize a learning rate scheduler with ReduceLROnPlateau\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "# Initialize the best validation loss to positive infinity\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Loop through epochs\n",
    "for epoch in range(n_epochs):\n",
    "    # Train the model on the training data\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                       batch_size, seq_len, clip, device)\n",
    "    \n",
    "    # Evaluate the model on the validation data\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                          seq_len, device)\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    # Save the model if the validation loss improves\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model/best-val-lstm_lm.pt')\n",
    "\n",
    "    # Print training and validation perplexity\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Traning\n",
    "\n",
    "The training process in the provided code snippet follows a standard procedure for training a language model. It begins by initializing the model architecture and setting hyperparameters such as the vocabulary size, embedding dimension, hidden dimension, number of layers, dropout rate, and learning rate. The model is then instantiated and moved to the desired device. The Adam optimizer is employed to optimize the model's parameters, and the CrossEntropyLoss criterion is used to compute the loss during training.\n",
    "\n",
    "The actual training loop consists of multiple epochs. In each epoch, the model is put into training mode, and the training data is divided into batches of sequences. The hidden state is reset at the start of each epoch. For each batch, the model parameters are zeroed out, and the forward pass is performed. The predicted probabilities for each token in the vocabulary are compared with the true next tokens, and the loss is calculated. The gradients are then computed using backpropagation, and the model parameters are updated using the optimizer. The loss is accumulated over the batches to compute the epoch loss.\n",
    "\n",
    "After each epoch, the model is put into evaluation mode, and the validation data is processed similarly to compute the validation loss. The learning rate scheduler is used to adjust the learning rate based on the validation loss, potentially reducing the learning rate if the validation loss plateaus. If the current validation loss is the best observed so far, the model parameters are saved.\n",
    "\n",
    "Throughout the training process, the train and validation perplexities are calculated and printed, representing how well the model predicts the training and validation data, respectively. The perplexity is a measure of how surprised the model is by the data and is obtained by exponentiating the loss. The goal of training is to minimize the loss and improve the model's ability to generate accurate predictions for sequences of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 106.254\n"
     ]
    }
   ],
   "source": [
    "# Load the best model state from the saved checkpoint\n",
    "model.load_state_dict(torch.load('model/best-val-lstm_lm.pt',  map_location=device))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "# Print the test perplexity\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    tokens = tokenizer(prompt)\n",
    "    \n",
    "    # Convert tokens to indices using the vocabulary\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    \n",
    "    # Set batch size to 1 for generation\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Initialize hidden states for the model\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    # Generate sequence\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            # Convert indices to a PyTorch LongTensor and move to the specified device\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            \n",
    "            # Get model predictions and update hidden states\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            # Softmax and temperature scaling for sampling\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            # If sampled token is '<unk>', sample again\n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # If sampled token is '<eos>', stop generation\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            # Append the sampled token to the generated sequence (autoregressive)\n",
    "            indices.append(prediction)\n",
    "\n",
    "    # Convert indices back to tokens using the vocabulary\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry missed\n",
      "\n",
      "0.7\n",
      "harry missed\n",
      "\n",
      "0.75\n",
      "harry missed\n",
      "\n",
      "0.8\n",
      "harry missed\n",
      "\n",
      "1.0\n",
      "harry missed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the generation prompt, maximum sequence length, and seed for reproducibility\n",
    "prompt = 'harry'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "# Set different temperatures for temperature scaling during generation\n",
    "# Smaller temperature values result in more diverse but potentially less coherent sequences\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "# Loop through different temperatures and generate sequences\n",
    "for temperature in temperatures:\n",
    "    # Generate a sequence using the specified temperature\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    \n",
    "    # Print the temperature, generated sequence, and a newline for separation\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming tokenized_dataset is your dataset with a 'train' split containing 'tokens'\n",
    "\n",
    "# Save the vocabulary using pickle\n",
    "with open('model/lstm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the web application interfaces with the language model\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Purpose of the Web Application](#11-purpose-of-the-web-application)\n",
    "    - [Overview of Features](#12-overview-of-features)\n",
    "\n",
    "2. [Architecture Overview](#2-architecture-overview)\n",
    "    - [Frontend Components](#21-frontend-components)\n",
    "    - [Backend Components](#22-backend-components)\n",
    "    - [Language Model Integration](#23-language-model-integration)\n",
    "\n",
    "3. [Components Description](#3-components-description)\n",
    "    - [Frontend Components](#31-frontend-components)\n",
    "        - [HTML/CSS Templates](#311-htmlcss-templates)\n",
    "        - [Flask Views](#312-flask-views)\n",
    "    - [Backend Components](#32-backend-components)\n",
    "        - [Flask Application](#321-flask-application)\n",
    "        - [Language Model Module](#322-language-model-module)\n",
    "\n",
    "4. [Data Flow](#4-data-flow)\n",
    "    - [User Input Processing](#41-user-input-processing)\n",
    "    - [Communication with the Language Model](#42-communication-with-the-language-model)\n",
    "    - [Displaying Results](#43-displaying-results)\n",
    "\n",
    "5. [Integration with Language Model](#5-integration-with-language-model)\n",
    "    - [Loading the Language Model](#51-loading-the-language-model)\n",
    "    - [Generating Text](#52-generating-text)\n",
    "    - [Handling Temperature Settings](#53-handling-temperature-settings)\n",
    "    - [Web Interface and Language Model Interaction](#54-web-interface-and-language-model-interaction)\n",
    "\n",
    "6. [User Interaction](#6-user-interaction)\n",
    "    - [A1: Similar Words](#61-a1-similar-words)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Purpose of the Web Application\n",
    "\n",
    "The web application serves as an interface for users to interact with a language model. It provides functionalities such as finding similar words and generating text based on user prompts.\n",
    "\n",
    "### 1.2 Overview of Features\n",
    "\n",
    "- **A2: Language Model Text Generation:** Enables users to generate text based on a prompt using a pre-trained LSTM language model.\n",
    "\n",
    "## 2. Architecture Overview\n",
    "\n",
    "### 2.1 Frontend Components\n",
    "\n",
    "- **HTML/CSS Templates:** Provide the structure and style for the web pages.\n",
    "- **Flask Views:** Define the routes and handle user requests.\n",
    "\n",
    "### 2.2 Backend Components\n",
    "\n",
    "- **Flask Application:** Manages the backend logic and communication between frontend and backend.\n",
    "- **Language Model Module:** Handles interactions with the pre-trained language model.\n",
    "\n",
    "### 2.3 Language Model Integration\n",
    "\n",
    "The web application integrates with a pre-trained language model for text generation. The model is loaded and used to generate text based on user prompts.\n",
    "\n",
    "## 3. Components Description\n",
    "\n",
    "### 3.1 Frontend Components\n",
    "\n",
    "#### 3.1.1 HTML/CSS Templates\n",
    "\n",
    "- `index.html`: Main landing page.\n",
    "- `a1.html`: Page for A1 functionality.\n",
    "- `a2.html`: Page for A2 functionality.\n",
    "\n",
    "#### 3.1.2 Flask Views\n",
    "\n",
    "- `home()`: Renders the main landing page.\n",
    "- `a1()`: Handles requests and renders A1 functionality.\n",
    "- `a2()`: Handles requests and renders A2 functionality.\n",
    "\n",
    "### 3.2 Backend Components\n",
    "\n",
    "#### 3.2.1 Flask Application\n",
    "\n",
    "- `app.py`: Main Flask application file.\n",
    "- `templates/`: Directory containing HTML templates.\n",
    "- `static/`: Directory containing CSS and other static files.\n",
    "\n",
    "#### 3.2.2 Language Model Module\n",
    "\n",
    "- `lstm.py`: Defines the LSTM language model class and related functions\n",
    "\n",
    "## 4. Data Flow\n",
    "\n",
    "### 4.1 User Input Processing\n",
    "\n",
    "1. User submits a form on the web page.\n",
    "2. Flask views process the form data.\n",
    "3. User input is sent to the backend for further processing.\n",
    "\n",
    "### 4.2 Communication with the Language Model\n",
    "\n",
    "1. Language model is loaded during the application startup.\n",
    "2. User input is passed to the language model for text generation.\n",
    "3. Generated text is returned to the backend.\n",
    "\n",
    "### 4.3 Displaying Results\n",
    "\n",
    "1. Backend sends the generated results to the frontend.\n",
    "2. Frontend dynamically updates the web page with the results.\n",
    "\n",
    "## 5. Integration with Language Model\n",
    "\n",
    "### 5.1 Loading the Language Model\n",
    "\n",
    "- The LSTM language model is loaded from a pre-trained checkpoint during the application startup.\n",
    "- Files needed best-val-lstm_lm.pt, lstm_model.pkl and vocab_lm.pkl (all in models folders)\n",
    "\n",
    "### 5.2 Generating Text\n",
    "\n",
    "- The language model is used to generate text based on user prompts, incorporating temperature settings for diversity.\n",
    "\n",
    "### 5.3 Handling Temperature Settings\n",
    "\n",
    "- Temperature settings are passed to the language model during text generation to control the randomness of the output.\n",
    "\n",
    "### 5.4 Web Interface and Language Model Interaction\n",
    "\n",
    "The web application interfaces with the language model in the following ways:\n",
    "\n",
    "- **A2 Functionality (Language Model Text Generation):**\n",
    "  - The user inputs a prompt.\n",
    "  - The backend passes the prompt to the pre-trained LSTM language model.\n",
    "  - Generated text is returned to the frontend for display.\n",
    "\n",
    "## 6. User Interaction\n",
    "\n",
    "### 6.1 A1: Similar Words\n",
    "\n",
    "- Users input a word in A1 and receive a list of words similar to the input, based on pre-trained embeddings.\n",
    "\n",
    "### 6.2 A2: Language Model Text Generation\n",
    "\n",
    "- Users input a prompt in A2 and receive generated text from the language model based on Harry Potter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
